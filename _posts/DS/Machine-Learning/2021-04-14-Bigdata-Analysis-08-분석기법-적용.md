---
layout: post
title: "08: 분석 기법 적용"
category: "DS/ML"
comments: true
tags: [Data Science, "Machine Learning", "빅데이터분석기사"]
feature-img: "assets/img/77.jpg"
feature-title: ""
use_math: true
summary: "분석 기법들에 대해 알아보자."
series: "빅데이터분석기사 - 필기"
---

# 회귀 분석

* 검증 방법
  * 통계적으로 유의미?
    * F-통계량
  * 회귀 계수가 유의미?
    * 해당 계수의 T-통계량, p-value
  * 설명력은?
    * 결정 계수 - 총 변동 중 회귀식이 설명하는 정도
  * 데이터를 잘 적합하는가?
    * 잔차를 그래프로 그린다.
  * 가정을 만족하는가?
    * 등분산성, 독립성, 선형성, 비상관성, 정상성
  * 다중공선성은 없는가?
    * VIF 값 확인


$$
R^2 = {SSR \over SST} = { \sum_{i=1}^n{(\hat{y_i}-\bar{y})^2}   \over \sum_{i=1}^n{(y_i-\bar{y})^2}}
$$

1에 가까울 수록 잘 설명하는 것임


# 로지스틱 회귀 분석

* y가 범주일 때 사용
* odds를 사용함
* 회귀 부호가 0보다 작다 -> 역 S
* 회귀 부호가 0보다 크다 -> S




# 의사결정나무

* 의사 결정 나무 성장
  * 분리 규칙에 따라 나무가 성장함
  * 분리 규칙
    * 이산형
      * 카이제곱 통계량 p 값
      * 지니 계수
      * 엔트로피 지수
    * 연속형
      * F-통계량
      * 분산 감소량
* 가지치기
  * 오류를 크게 할 위험이 높은 불필요한 가지를 제거함
* 타당성 평가
  * **이익 도표**, 위험 도표, 시험 자료를 이용하여 평가 진행
* 해석 및 예측

* 부모 마디
* 중간 마디
* 자식 마디
## 불순도 계산


![image](https://user-images.githubusercontent.com/37871541/114745230-71304000-9d89-11eb-8974-485a776761c5.png){: .center-small}


## 의사결정나무 알고리즘

R에서 party 패키지에서 사용가능함.

* CART(Classification And Regression Tree)
  * 가장 널리 사용됨
  * 범주형인 경우 **지니 지수**, 연속형인 경우 이진 분리
* C4.5, C5.0
  * 목표 변수는 **범주형이어야만 함**
  * 다지 분리 가능
  * **엔트로피 지수**만 가능
* CHAID
  * AID 발전 알고리즘
  * **가지치기 하지 않음.** 적당한 크기에서 중지
  * 입력 변수가 범주형이어야만 함
  * 분리 기준 : 불순도로 카이제곱 통계량 사용
  * 다지 분리 : 다지 분리
* QUEST
  * CART 개선
  * 변수 편향 없음
  * 불순도로 **카이제곱 통계량 사용**


![image](https://user-images.githubusercontent.com/37871541/114745892-19460900-9d8a-11eb-8364-c1f11fb62f1a.png){: .center-small}_알고리즘 요약표_


## 의사결정나무 활용

* 세분화(Segmentation) - 그룹 분할
* 분류
* 예측
* 차원 축소 및 변수선택 - 목표 변수에 영향 미치는 변수만 선택함
* 교호 작용 효과의 파악 - 직관적인 알고리즘이기 때문에 규칙 파악이 가능함

## 의사결정나무의 장단점

* 장점
  * 해석의 용이성
  * 상호작용 효과의 해석 가능
  * 비모수적 모형
    * 가정이 필요없이 알고리즘으로 돌아감
    * 이상값에 민감하지 않음
  * 유연성
    * 빠르게 만들 수 있음
  * 높은 정확도
* 단점
  * 비연속성
    * 분리의 경계점 근방에서 예측 오류가 클 가능성 있음. (연속형 변수를 비연속적으로 취급)
  * 선형성 결여
  * 비안정성
    * 학습 자료에 지나치게 의존함

# 인공 신경망

* 1세대
  * 퍼셉트론
  * 순방향 신경말
  * XOR 문제
* 2세대
  * 다층 퍼셉트론
  * 역전파
  * 과적합 문제
  * 기울기 소실


# SVM

* 커널 함수 니맘대로 설정
* 커널 함수를 사용하면 저차원에서 고차원으로 매핑할 경우 증가하는 연산량 커버 가능
* 훈련속도 느림
* 선형 분리 어려운 경우는 차원을 고차원으로 매핑하여 해결
* 과대 적합 가능성 낮음
* 슬랙 변수를 통해 소프트하게 선형적으로 분리를 돕는다.


# 연관 분석

* 지지도
  * $P(A \cap B)$
* 신뢰도
  * A를 샀을 때 B를 살 신뢰도
  * $P(A \cap B) \over P(A)$
* 향상도
  * 두 품목의 상관관계를 기준으로 도출된 규칙의 예측력을 평가
  * 1보다 크면 좋다고 판단
  * $P(B \mid A) \over P(B)$
  * $신뢰도 \over P(B)$
  * $P(A \cap B) \over P(A)P(B)$


# 군집 분석




n개의 군집으로 나누어 집단 특성을 분석하는 방법

## 계층적 군집

![image](https://user-images.githubusercontent.com/37871541/114747494-dd13a800-9d8b-11eb-9a26-113c88abf85b.png){: .center-small}


* 병합적 방법
  * 작은 군집으로부터 시작
  * 거리가 가까우면 유사성이 높음
  * `stats` 패키지 `hclust()`
  * `cluster` 패키지 `mclust()`
* 분할적 방법
  * 분리해 나가는 방법
  * `cluster` 패키지 `diana()`, `mona()`
* 계통도(덴드로그램)
* 거리측정 방법
  * 최단연결법 - 군집 관측값끼리의 최단 거리
  * 최장연결법 - 군집 관측값끼지의 최장 거리
  * 중심연결법 - 군집 중심 간의 거리
  * 평균연결법 - 모든 항목에 대한 거리 평균을 구함
  * 와드연결법 - 군집 내 오차 제곱합에 기초하여 군집 수행
* 거리 종류
  * 연속형
    * 수학적 거리
      * 유클리드 거리
      * 맨하튼 거리
      * 민코프스키 거리
    * 통계적 거리
      * 표준화 거리
      * 마할라노비스 거리
  * 명목형
    * 단순 일치 계수
    * 자카드 계수 - 집합 사이의 유사도 측정
  * 순서형
    * 순위상관계수





# K-means - 생략

# EM 알고리즘 - 생략

# SOM(Self-Organizing Maps)


![image](https://user-images.githubusercontent.com/37871541/114751075-a93a8180-9d8f-11eb-84f3-e7830b709c8d.png){: .center-small}


* 대뇌피질, 시각피질 학습과정
* 고차원 데이터를 저차원의 뉴런으로 정렬, 지도의 형태
* 위치관계를 그대로 보존함
* 실제 공간의 입력변수가 가까이 있으면 지도 상에 가까운 위치에 있다.
* 입력층
  * 입력변수와 동일하게 뉴런 개수 존재
* 경쟁층
  * 2차원 격자로 클러스터링되는 층
  * 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가 계산한다.
  * 경쟁 과정을 거치며 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 됨
  * 경쟁층에는 승자 독식 구조로 인해 승자 뉴런만이 나타남


# 범주형 자료 분석

* 상대위험도
  * $관심집단의위험률 \over 비교집단의위험률$
  * ${a \over {a+b}} / {c \over {c+d}}$
* Odds
  * $p \over {1-p}$
* Odds Ratio
  * $관심집단의승산 \over 비교집단의승산$
  * 관심집단의 승산 = ${a \over {a+b}} / {b \over {a+b}} ={a \over b}$
  * 비교집단의 승산 = ${c \over {c+d}} / {d \over {c+d}} ={c \over d}$

# 시계열 분석

* 정상성
  * 시점에 상관없이 특성이 일정하다는 의미
  * 평균 일정
  * 분산이 시점에 의존하지 않는다.
  * 공분산은 시차에만 의존
* 모형
  * 자기 회귀 모형 (AR)
    * 현재가 p개의 이전 자료로 설명 가능
  * 이동평균모형(Moving Average Model)
    * 시간이 지날수록 평균값이 지속적인 증가 혹은 감소
    * 정상성 가정 필요 없음
  * 자기 회귀 누적 이동평균모형(ARIMA)
    * 비정상 시계열 모델
    * 차분, 변환을 통해 AR, MA, ARMA 모형으로 정상화 가능
    * ARIMA(p, d, q)
      * p : AR 모형
      * q : MA 모형
      * d : ARIMA -> ARMA 과정에서 차분의 횟수
  * 분해 시계열
    * 추세 요인
      * 선형적 추세, 이차식 형태, 지수적 형태
    * 계절 요인
      * 요일마다 반복, 월에 의한 변화, 분기마다 변화
    * 순환 요인
      * 잘 모르는 주기를 가지고 변화
    * 불규칙 요인
      * 위의 세 요인으로 설명할 수 없는 오차



# 베이지안 기법 - 생략



# 딥러닝 - 생략



# 앙상블 분석 - 생략


# 비모수 통계

* 단일 표본 부호 검정
   *  부호 검정
      *  중위수와의 차이 부호만을 이용하여 중위수 위치 검정
      *  가정 : 연속적이고 독립적인 분포에서 나왔는가?
* 단일 표본 부호 순위 검정
  * 윌콕슨 부호 순위 검정
    * 단일 표본 중위수 검정에 사용
    * 두개 표본의 중위수 차이 검정에 사용
    * 부호 검정에서 부호 뿐만 아니라 차이도 고려함
    * 가정 : 연속적이고 독립적인 분포에서 나왔는가?, 대칭성
* 두 표본 검정
  * 윌콕슨 순위 합 검정
    * 두 표본 중위수 검정
* 대응 표본 검정
  * 같은 표본에서 다른 처리를 한 두 집단의 중위수 차이를 검정함
* 분산 분석
  * 크루스칼 왈리스 검정
    * 세 집단 이상의 분포 비교
    * 평균이 아닌 중위수가 같은지 검점
* 런 검정
  * 두 개의 값(앞면, 뒷면)을 가지는 연속적인 측정값(101001)이 패턴이 있는지를 검정
  * 혹은 랜덤하게 추출되었는지 검증하고 싶을 때 사용
  * 1/0/1/00/1로 끊고, 5개의 연속적인 런이라 함


## 비지도 학습

* 다차원 척도법
  * 자료의 관계를 파악하는 시각화 방법
  * 거리가 주어져 있을 때, 동일한 상대적 거리를 가진 실수 공간의 점들로 배치
* 주성분 분석
  * 누적 기여율