---
layout: post
title: "08: 분석 기법 적용"
category: "DS/ML"
comments: true
tags: [Data Science, "Machine Learning", "빅데이터분석기사"]
feature-img: "assets/img/77.jpg"
feature-title: ""
use_math: true
summary: "분석 기법들에 대해 알아보자."
series: "빅데이터분석기사 - 필기"
---

# 회귀 분석

* 검증 방법
  * 통계적으로 유의미?
    * F-통계량
  * 회귀 계수가 유의미?
    * 해당 계수의 T-통계량, p-value
  * 설명력은?
    * 결정 계수
  * 데이터를 잘 적합하는가?
    * 잔차를 그래프로 그린다.
  * 가정을 만족하는가?
    * 등분산성, 독립성, 선형성, 비상관성, 정상성
  * 다중공선성은 없는가?
    * VIF 값 확인


$$
R^2 = {SSR \over SST} = { \sum_{i=1}^n{(\hat{y_i}-\bar{y})^2}   \over \sum_{i=1}^n{(y_i-\bar{y})^2}}
$$

1에 가까울 수록 잘 설명하는 것임


# 로지스틱 회귀 분석

* y가 범주일 때 사용
* odds를 사용함


# 의사결정나무

* 의사 결정 나무 성장
  * 분리 규칙에 따라 나무가 성장함
  * 분리 규칙
    * 이산형
      * 카이제곱 통계량 p 값
      * 지니 계수
      * 엔트로피 지수
    * 연속형
      * F-통계량
      * 분산 감소량
* 가지치기
  * 오류를 크게 할 위험이 높은 불필요한 가지를 제거함
* 타당성 평가
  * 이익 도표, 위험 도표, 시험 자료를 이용하여 평가 진행
* 해석 및 예측

## 불순도 계산


![image](https://user-images.githubusercontent.com/37871541/114745230-71304000-9d89-11eb-8974-485a776761c5.png){: .center-small}


## 의사결정나무 알고리즘

R에서 party 패키지에서 사용가능함.

* CART(Classification And Regression Tree)
  * 가장 널리 사용됨
  * 범주형인 경우 지니 지수, 연속형인 경우 이진 분리
* C4.5, C5.0
  * 목표 변수는 **범주형이어야만 함**
  * 다지 분리 가능
* CHAID
  * AID 발전 알고리즘
  * 가지치기 하지 않음. 적당한 크기에서 중지
  * 입력 변수가 범주형이어야만 함
* QUEST
  * CART 개선
  * 변수 편향 없음
  * 불순도로 카이제곱 통계량 사용


![image](https://user-images.githubusercontent.com/37871541/114745892-19460900-9d8a-11eb-8364-c1f11fb62f1a.png){: .center-small}_알고리즘 요약표_


## 의사결정나무 활용

* 세분화(Segmentation) - 그룹 분할
* 분류
* 예측
* 차원 축소 및 변수선택 - 목표 변수에 영향 미치는 변수만 선택함
* 교호 작용 효과의 파악 - 직관적인 알고리즘이기 때문에 규칙 파악이 가능함

## 의사결정나무의 장단점

* 장점
  * 해석의 용이성
  * 상호작용 효과의 해석 가능
  * 비모수적 모형
    * 가정이 필요없이 알고리즘으로 돌아감
    * 이상값에 민감하지 않음
  * 유연성
    * 빠르게 만들 수 있음
  * 높은 정확도
* 단점
  * 비연속성
  * 선형성 결여
  * 비안정성
    * 학습 자료에 지나치게 의존함

# 인공 신경망 - 생략


# SVM - 생략


# 연관 분석

* 지지도
  * $P(A \cap B)$
* 신뢰도
  * A를 샀을 때 B를 살 신뢰도
  * $P(A \cap B) \over P(A)$
* 향상도
  * $P(B \mid A) \over P(B)$
  * $신뢰도 \over P(B)$
  * $P(A \cap B) \over P(A)P(B)$


# 군집 분석




n개의 군집으로 나누어 집단 특성을 분석하는 방법

## 계층적 군집

![image](https://user-images.githubusercontent.com/37871541/114747494-dd13a800-9d8b-11eb-9a26-113c88abf85b.png){: .center-small}


* 병합적 방법
  * 작은 군집으로부터 시작
  * 거리가 가까우면 유사성이 높음
  * `stats` 패키지 `hclust()`
  * `cluster` 패키지 `mclust()`
* 분할적 방법
  * 분리해 나가는 방법
  * `cluster` 패키지 `diana()`, `mona()`
* 계통도(덴드로그램)
* 거리측정 방법
  * 최단연결법 - 군집 관측값끼리의 최단 거리
  * 최장연결법 - 군집 관측값끼지의 최장 거리
  * 중심연결법 - 군집 중심 간의 거리
  * 평균연결법 - 모든 항목에 대한 거리 평균을 구함
  * 와드연결법 - 군집 내 오차 제곱합에 기초하여 군집 수행
* 거리 종류
  * 연속형
    * 수학적 거리
      * 유클리드 거리
      * 맨하튼 거리
      * 민코프스키 거리
    * 통계적 거리
      * 표준화 거리
      * 마할라노비스 거리
  * 명목형
    * 단순 일치 계수
    * 자카드 계수 - 집합 사이의 유사도 측정
  * 순서형
    * 순위상관계수





# K-means - 생략

# EM 알고리즘 - 생략

# SOM(Self-Organizing Maps)


![image](https://user-images.githubusercontent.com/37871541/114751075-a93a8180-9d8f-11eb-84f3-e7830b709c8d.png){: .center-small}


* 대뇌피질, 시각피질 학습과정
* 고차원 데이터를 저차원의 뉴런으로 정렬, 지도의 형태
* 위치관계를 그대로 보존함
* 실제 공간의 입력변수가 가까이 있으면 지도 상에 가까운 위치에 있다.
* 입력층
  * 입력변수와 동일하게 뉴런 개수 존재
* 경쟁층
  * 2차원 격자로 클러스터링되는 층
  * 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가 계산한다.
  * 경쟁 과정을 거치며 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 됨
  * 경쟁층에는 승자 독식 구조로 인해 승자 뉴런만이 나타남




# 시계열 분석




# 베이지안 기법 - 생략



# 딥러닝 - 생략



# 앙상블 분석 - 생략


# 비모수 통계