<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>완숙의 에그머니🍳</title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://localhost:4000/</link>
    <description>얼떨결에 들어왔으니 이것도 인연😌</description>
    <pubDate>Tue, 08 Sep 2020 14:20:14 +0900</pubDate>
    
      <item>
        <title>13: Mask R-CNN</title>
        <link>/ds/dl/2020/09/07/computer-vision-13-Mask-RCNN.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-13-Mask-RCNN.html</guid>
        <description>&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-60-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;structure of mask RCNN&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Facebok AI Research (FAIR), Kaiming He, 24 Jan 2018&lt;/li&gt;
  &lt;li&gt;Marr Prize at ICCV 2017&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;object instance segmentation&lt;/code&gt;을 위한 프레임 워크이다. 기존의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;semantic segmentation&lt;/code&gt;을 넘어서 각각의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instance&lt;/code&gt;도 구분이 가능한 구조를 만들었다. 학습이 쉽고 Faster RCNN에 조금의 overhead만 추가하여 5fps의 빠르기로 실행된다. COCO 데이터셋에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instance segmentation&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bbox object detection&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person keypoint detection&lt;/code&gt; 에서 가장 높은 결과를 보였다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-아이디어&quot;&gt;핵심 아이디어&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Faster R-CNN에서 detect한 각각의 box에 mask를 씌워주자!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instance segmentation&lt;/code&gt;은 두 가지 과제를 합친 것이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;object detection
    &lt;ul&gt;
      &lt;li&gt;bbox를 이용하여 object를 분류하고, 위치를 찾는 것.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;semantic segmentation
    &lt;ul&gt;
      &lt;li&gt;object instance는 구별하지 않지만, 정해진 카테고리별로 각각의 pixel을 분류하는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이전의 Segmentation에서 중요한 논문인 FCN에서는 총 3가지를 고려하였다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;pixel 단위의 classification&lt;/li&gt;
  &lt;li&gt;그렇기 때문에 pixel 단위 softmax 값 추출이 필요&lt;/li&gt;
  &lt;li&gt;multi instance를 고려해야 함&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;하지만 mask RCNN은 Faster RCNN을 그대로 가져다가 쓰기 때문에, 이 문제가 다소 변경된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;del&gt;pixel 단위의 classification&lt;/del&gt; -&amp;gt; 이미 bounding box로 구분을 해줌&lt;/li&gt;
  &lt;li&gt;&lt;del&gt;그렇기 때문에 pixel 단위 softmax 값 추출이 필요&lt;/del&gt; -&amp;gt; bounding box 안에서 물체 인지 아닌지만 구분해주면 됨(Sigmoid)&lt;/li&gt;
  &lt;li&gt;&lt;del&gt;multi instance를 고려해야 함&lt;/del&gt; -&amp;gt; 이미 multi instance로 bounding box를 쳐줌&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-18-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;class, box 외에 mask FCN만 추가한다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;결과적으로, 이 문제에서 해야할 일은 masking을 수행하는 것이다. 그래서 논문이름도 Mask RCNN이다.&lt;/p&gt;

&lt;h2 id=&quot;equivariance&quot;&gt;Equivariance&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;input에서의 변화가 output의 변화에 영향을 준다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-39-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Invariance vs. Equivariance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;classification 문제에서는 label을 도출하는 문제이기 때문에 Invariance 하다. 하지만 segmentation 문제 같은 경우에는 output이 원래 이미지 사이즈와 같아야 하기 때문에 이 문제는 Equivariance로 해결 해야한다. 이 때, 저자들은 convolution은 translation-equivariance 하기 때문에 이 네트워크를 사용했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-41-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-42-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Fully convolutional network 사용&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;mask RCNN의 구조를 담당하는 Faster RCNN은 Fully conv net을 사용하고 있다. 여기서 mask RCNN은 뒤의 mask head부분 역시 FCN을 사용하여 제작하였다.&lt;/p&gt;

&lt;h2 id=&quot;roi-align&quot;&gt;RoI Align&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-47-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;기존의 Faster RCNN의 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;기존에 Faster RCNN에서는 feature map을 뽑아낸 뒤, Region proposal Network를 사용하여 이를 제시하였다. 그 방법은 RoI pooling이었다. 하지만 segmentation은 detection 문제와 다르게 단지 box를 치는 문제가 아니다. 좀더 정확한 위치정보를 담은 상태의 feature map이 필요하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-54-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;RoI pooling은 proposal의 위치를 반올림한다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;기존의 RoI Pooling을 생각해보면, 4개의 좌표 변환 값을 regression하고, 이를 기반으로 예상 좌표를 얻어낸 뒤(실수) 이를 반올림하여 정수단위인 pixel의 위치를 제안한다. 하지만, 소수점을 반올림한 좌표를 가지고 Pooling을 해주면 input image의 원본 위치 정보가 왜곡된다. classfication에는 이런 문제가 심각하지 않지만, pixel-by-pixel로 detection을 진행해야 하는 segmentation 에서는 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-58-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;RoI Align&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 저자들은 다음과 같은 방법을 통해 이를 해결한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;제안된 proposal을 들고온다.&lt;/li&gt;
  &lt;li&gt;Roi pooling에서 4등분 했던 것처럼 일단 자른다.&lt;/li&gt;
  &lt;li&gt;그 안에서 추가적으로 4등분을 한다. (subcell)&lt;/li&gt;
  &lt;li&gt;이렇게 발생한 격자내에 들어오는 픽셀의 면적을 기준으로 가중평균한다.&lt;/li&gt;
  &lt;li&gt;발생한 값을 기준으로 pooling한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 방법은 Mask Accuracy에서 큰 향상을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;mask-rcnn-architecture&quot;&gt;Mask RCNN architecture&lt;/h2&gt;

&lt;p&gt;Mask R-CNN은 여러 가지 아키텍쳐를 합친 네트워크인데, 크게 두 가지로 나뉜다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Convolutional backbone architecture
    &lt;ul&gt;
      &lt;li&gt;이미지에서 feature extraction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Network head
    &lt;ul&gt;
      &lt;li&gt;bounding-box 인식(classification &amp;amp; regression), mask 예측&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-64-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Head Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resnet-backbone&quot;&gt;ResNet Backbone&lt;/h3&gt;

&lt;p&gt;논문에서는 ResNet 과 ResNeXt networks 를 depth 50 or 101 layers에 대해 평가했다. 원래 Faster R-CNN은 ResNet을 사용하는데, 4번째 스테이지의 마지막 Conv layer(이하 C4)에서 features를 뽑아낸다.
이 경우, 이 backbone을 사용한다면 우리는 ResNet-50-C4 와 같이 부를 것이다. ResNet-50-C4가 일반적으로 사용된다.&lt;/p&gt;

&lt;h3 id=&quot;resnet-fpn-backbone&quot;&gt;ResNet-FPN Backbone&lt;/h3&gt;

&lt;p&gt;FPN은 Feature Pyramid Network로, top-down architecture를 사용한다. FPN backbone을 사용하는 Faster R-CNN은 피쳐 피라미드의 서로 다른 레벨로부터 RoI features를 뽑아내지만, 나머지는 vanilla ResNet과 같다. Mask R-CNN에서 피쳐 추출을 위해 ResNet-FPN backbone을 이용하는 것은 정확도와 속도 면에서 엄청난 향상을 보였다. Feature Pyramid Network는 추후 글에서 작성하도록 하겠다.&lt;/p&gt;

&lt;h2 id=&quot;loss-function-decoupling&quot;&gt;Loss function (decoupling)&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = L_{cls} + L_{box} + L_{mask}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$L_{cls}$ : Softmax Cross Entropy (loss of classification)&lt;/li&gt;
  &lt;li&gt;$L_{box}$ : bbox regression&lt;/li&gt;
  &lt;li&gt;$L_{mask}$ : Binary Cross Entropy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 아이디어를 그대로 가져와서, 결과적으로 masking만 하는 loss 함수를 정의하여 사용한다. 그림으로 이해해 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-27-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;이전 방법들과의 비교&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그림을 보게되면, 단순히 masking을 하는 구조를 추가하고, 이를 반영하는 방식으로 진행된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-35-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;mask Head의 loss update 방법&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;update 방법은 상당히 단순한데, 일단 전체 mask loss는 모든 클래스(사람, 말 등)에서 차이가 나는 mask의 정도로 정의가 된다. 하지만 해당 사진에서 bounding box는 하나만 box 처리가 되어 있다. 기존의 faster RCNN에서 bounding box를 예측할 때는 하나의 box만 처리하기 때문이다. 이런 상황에서 mask에 대한 업데이트는 모든 사물에 대해서 업데이트를 할 수 없게 된다. 그래서 이렇게 해당 사진의 class가 정해질 경우, 해당 class에 해당하는 mask만을 선택하고 이를 업데이트 해준다. 즉, 말이 정답 class인 경우, 이 class에 해당하는 mask만 학습된다.&lt;/p&gt;

&lt;p&gt;결과적으로 이렇게 학습되는 mask branch는 &lt;strong&gt;어떠한 class인지 상관 없이 물체의 masking만 따는 것을 배우게 된다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-37-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;test senario&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이렇게 학습된 mask branch는 실제로 여러개의 물체에 대한 mask라고 예측할 것이다. 하지만 이 녀석은 어떠한 물체인지 분간하지 못하는데, 이부분에 있어서 classification의 결과를 넣어주어, 하나의 masking을 제안한다. 즉 mask prediction에서는 단지 이 pixel이 mask인지, 아닌지 만을 구분(sigmoid 사용)하도록 하여 성능의 향상을 보였다. 이러한 방법을 Mask prediction 과 class prediction 을 decouple 했다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr057maskrcnn-180107092616/95/pr057-mask-rcnn-38-638.jpg?cb=1515317235&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;decouple을 시도했을 때 올라간 정확도&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://mylifemystudy.tistory.com/82&quot;&gt;Mask R-CNN 정리&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=RtSZALC9DlU&amp;amp;t=248s&quot;&gt;PR-057: Mask R-CNN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>12: Deep Lab v3</title>
        <link>/ds/dl/2020/09/07/computer-vision-12-deep-lab-v3.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-12-deep-lab-v3.html</guid>
        <description>&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>11: Deep Lab v2</title>
        <link>/ds/dl/2020/09/07/computer-vision-11-deep-lab-v2.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-11-deep-lab-v2.html</guid>
        <description>&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>10: Deep Lab v1</title>
        <link>/ds/dl/2020/09/07/computer-vision-10-deep-lab-v1.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-10-deep-lab-v1.html</guid>
        <description>&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>09: Conditional Random Field(CRF)</title>
        <link>/ds/dl/2020/09/07/computer-vision-09-conditional-random-field.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-09-conditional-random-field.html</guid>
        <description>&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>08: Dilated Convolution</title>
        <link>/ds/dl/2020/09/07/computer-vision-08-Dilated-Convolution.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-08-Dilated-Convolution.html</guid>
        <description>&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;p&gt;앞서 알아본, FCN에서 발생하는 문제를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dilated Convolution&lt;/code&gt;으로 해결하겠다는 논지이다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-아이디어&quot;&gt;핵심 아이디어&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dilated Convolution으로 parameter 수를 유지하면서 Pooling의 효과를 누리고, Resoultion이 줄어드는 것을 막는다.(&lt;del&gt;일석삼조&lt;/del&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;기존의 FCN에서는 pooling으로 인한 해상도 감소문제를 skip connection으로 해결하였다. 여기서, 근본적으로 pooling에 대해 해결하보려는 의지가 엿보인다. 우리가 Pooling을 하는 이유는 global feature를 multi-scale로 보기 위해서이다. 하지만 이런 관점은 classification의 관점에서 맞는 말이다.&lt;/p&gt;

&lt;p&gt;segementation을 위해서는 결국 dense prediction을 얻어내야 하는데, 이는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;up-convolutions&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multi-scale inputs&lt;/code&gt;를 통해 가능하다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;up-convolutions&lt;/code&gt;는 &lt;a href=&quot;https://wansook0316.github.io/ds/dl/2020/09/07/computer-vision-07-Learning-Deconvolutional-Network-for-Semantic-Segmentation.html&quot;&gt;이전 글&lt;/a&gt;에서 찾아볼 수 있다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multi sclae inputs&lt;/code&gt;은 이름에서도 유추가 가능하듯이 하나의 이미지에 대해서 여러 scale에서 test를 하는 방법을 의미한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://hoya012.github.io/assets/img/object_detection_sixth/1.PNG&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;multi scale inputs 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dilated Convolution&lt;/code&gt;은 애초에 pooling을 해야돼? 라는 질문에서 출발한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99448C335A014DD609&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;Dilated Convolution&lt;/em&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dilated Convolution&lt;/code&gt;은 필터 내부에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zero padding&lt;/code&gt;을 추가해 강제로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receptive field&lt;/code&gt;를 늘리는 방법이다. 위 그림은 파란색이 인풋, 초록색이 아웃풋인데, 진한 파랑 부분에만 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weight&lt;/code&gt;가 있고 나머지 부분은 0으로 채워진다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receptive field&lt;/code&gt;란 필터가 한 번의 보는 영영으로 볼 수 있는데, 결국 필터를 통해 어떤 사진의 전체적인 특징을 잡아내기 위해서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receptive field&lt;/code&gt;는 높으면 높을 수록 좋다. 그렇다고 필터의 크기를 크게하면 연산의 양이 크게 늘어나고, 오버피팅의 우려가있다.&lt;/p&gt;

&lt;p&gt;그래서 일반적인 CNN에서는 이를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conv-pooling&lt;/code&gt;의 결합으로 해결한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pooling&lt;/code&gt;을 통해 dimension을 줄이고 다시 작은 크기의 filter로 conv를 하면, 전체적인 특징을 잡아낼 수 있다. 하지만 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pooling&lt;/code&gt;을 수행하면 기존 정보의 손실이 일어난다. 이를 해결하기 위한것이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dilated Convolution&lt;/code&gt;으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pooling&lt;/code&gt;을 수행하지 않고도 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receptive field&lt;/code&gt;의 크기를 크게 가져갈 수 있기 때문에 spatial dimension의 손실이 적고, 대부분의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weight&lt;/code&gt;가 0이기 때문에 연산의 효율도 좋다.&lt;/p&gt;

&lt;h2 id=&quot;structure&quot;&gt;Structure&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99BD2B335A01526610&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Structure of using Dilated Convolution&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;첫번째 그림은 classification을 위한 CNN VGG-16의 아키텍쳐이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conv-pooling&lt;/code&gt;을 반복적으로 수행한 후, 마지막으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fully Connected Layer&lt;/code&gt;에 통과하여 최종 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt; 결과를 얻는 과정을 보여주고있다. 그 아래의 그림은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dilated Convolution&lt;/code&gt;을 통하여 이미지를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;segmentation&lt;/code&gt;하는 예를 보여주고 있다. 이 아키텍쳐의 아웃풋의 사이즈는 28x28xN 이며, (N은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;segmentation&lt;/code&gt; 원하는 클래스의 수) 이를 다시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsampling&lt;/code&gt;하여 원래의 크기로 복원한다. (이부분에서 공간적 정보의 손실이 있다.)&lt;/p&gt;

&lt;p&gt;이 아키텍쳐와 classification 아키텍쳐의 다른점은 우선 다이아몬드 모양으로 표시한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dilated convolution&lt;/code&gt;으 통해 공간적 정보의 손실을 최소화하였다. 그리고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dilated convolution&lt;/code&gt; 2번을 적용한 뒤 나온 28x28x4096 에 대하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1x1 convolution&lt;/code&gt;으로 channel의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dimension reduction&lt;/code&gt;을 수행한다. 최종적으로 28x28xN이 나오고 이를 8x &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsampling&lt;/code&gt;하여 최종적인 segmention 결과를 output으로 내놓는다. 이 때 1x1 convolution 은 공간적인 정보를 잃지 않기 위해 사용되며, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt;의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fully Connected Layer(FC)&lt;/code&gt;와 비슷한 역할을 한다. 하지만 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt;에서는 공간적인 정보는 중요하지 않기 때문에 Flatten하여 사용하는 것이다. 이는 &lt;a href=&quot;https://wansook0316.github.io/ds/dl/2020/09/07/computer-vision-06-Fully-Convolutional-Networks.html&quot;&gt;앞선 글&lt;/a&gt;에서 자세하게 다뤄보았다.&lt;/p&gt;

&lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99E713335A01509E2A&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;comparison of whether using dilated conv&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이 그림을 통해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pooling-conv&lt;/code&gt;후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsampling&lt;/code&gt;을 하는 것과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dilated convolution(astrous convolution)&lt;/code&gt;을 하는 것의 차이를 볼 수 있다. 위 그림에서 볼 수 있듯 공간적 정보의 손실이 있는 것을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsampling&lt;/code&gt; 하면 해상도가 떨어진다. 하지만 dilated convolution의 그림을 보면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receptive field&lt;/code&gt;를 크게 가져가면서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;convolution&lt;/code&gt;을 하면 정보의 손실을 최대화하면서 해상도는 큰 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;output&lt;/code&gt;을 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://modulabs-biomedical.github.io/Learning_Deconvolution_Network_for_Semantic_Segmentation&quot;&gt;Learning Deconvolution Network for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>07: Learning Deconvolution Network for Semantic Segmentation</title>
        <link>/ds/dl/2020/09/07/computer-vision-07-Learning-Deconvolutional-Network-for-Semantic-Segmentation.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-07-Learning-Deconvolutional-Network-for-Semantic-Segmentation.html</guid>
        <description>&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Noh, H., Hong, S., and Han, B. Learning Deconvolution Network for Semantic Segmentation. ICCV, 2015.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이번 논문은 앞서 다뤘던 Fully Convolutional Networks와 같은 년도(2015)에 다른 학회(FCN은 CVPR, 본 논문은 ICCV)에 발표된 논문이다. FCN이나 이후에 다룰 UNet보다는 다소 인기가 적었지만, FCN이 가진 한계를 잘 짚어주셨다는 점에서 의의가 있다.&lt;/p&gt;

&lt;h2 id=&quot;핵심-아이디어&quot;&gt;핵심 아이디어&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;크기, 디테일에 약해? -&amp;gt; layer 를 추가하자. (Upconvolution)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;크기의-문제점&quot;&gt;크기의 문제점&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig1.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 예시들처럼 FCN의 추론 결과를 보면, 대상 물체가 너무 큰 경우(a)에는 파편화되고, 너무 작은 경우(b)에는 배경으로 무시되는 경향이 있다. FCN에서는 receptive field(상위 레이어의 한 지점에서 참조하는 하위 레이어의 영역)의 크기가 고정되어, 단일 배율(scale)만을 학습하는 것이 이 문제의 원인이라고 본 논문은 지적한다. 여러 레이어의 결과를 조합하는 skip 구조가 이러한 현상을 완화시켜주기는 하지만, 근본적인 해법은 아니라는 주장한다.&lt;/p&gt;

&lt;h3 id=&quot;디테일의-문제점&quot;&gt;디테일의 문제점&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig5.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FCN이 비록 기존 기법들에 비해 큰 발전을 이루었지만, 세부적인 영역을 찾아내는 데에서는 아직 개선의 여지가 있다고 이 논문은 보고 있다. FCN에서는 deconvolution에 들어가는 입력부터 이미 세부 묘사가 떨어지고, deconvolution 과정 자체도 충분히 깊지 않고 너무 단순하다고 말한다.&lt;/p&gt;

&lt;h2 id=&quot;네트워크-구조의-변경&quot;&gt;네트워크 구조의 변경&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig2.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;부족하면 더 넣으면 된다. FCN에서는 CNN의 결과를 입력이미지의 원래 차원으로 확대(upsampling)하는데 있어서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deconvolution&lt;/code&gt;을 사용했지만, 이 논문에서는 deconvolution시 차원을 유지하는 방법으로, CNN의 layer만큼 레이어 숫자를 늘렸다. 즉, 완전한 대칭 모양이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig3.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;uppooling&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;CNN으로 인해 원래 이미지보다 축소된 차원 크기는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uppooling&lt;/code&gt;으로 복원합니다. 여기서 unpooling이란 CNN의 max pooling 시의 위치 정보를 기억했다가, 원래 위치로 그대로 복원해주는 작업이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig4.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;uppooling 과정 (a-&amp;gt;b)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그 효과는 위의 그림과 같다. (b)에서 (c)로 갈 때의 unpooling에 의해, 해상도가 커지는 대신 신호가 흩어져서 희소(sparse)해진다. 이것을 (c)에서 (d)로 deconvolution을 거치면, 디테일을 살려내면서 신호가 고르게 밀집(dense)된다. 이러한 과정이 반복되자 노이즈도 점차 자연스럽게 사라지는 것을 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;학습-방법&quot;&gt;학습 방법&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/edge-box.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;edge-box&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;단일 데이터셋에서 다양한 크기의 사례들을 학습하기 위해, 논문에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edge-box&lt;/code&gt;라는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;object proposal&lt;/code&gt; 알고리즘을 사용하여 무언가 있을만한 영역을 다양한 크기의 상자로 골라낸다. 학습 시에는 우선 실제 정답이 가운데에 들어가도록 잘라낸(crop) 이미지들로 1차 학습을, 그 다음 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edge-box&lt;/code&gt;의 결과물 중 실제 정답과 잘 겹치는 것들을 활용하여 조금 더 심도있는 2차 학습을 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig6.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;edge-box inference&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이렇게 학습에 사용된 edge-box는 추론 시에도 사용되는데, 추론 시 사용하는 object proposal의 수(상자 수)를 증가시킬 수록 성능은 좋아진다고 한다. 물론 그만큼 계산량과 시간은 늘어난다.&lt;/p&gt;

&lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://modulabs-biomedical.github.io/assets/images/posts/2018-01-03-Learning_Deconvolution_Network_for_Semantic_Segmentation/fig7.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;result&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이렇게 세심하게 설계되고 학습된 결과는 FCN이 실수하는 물체들도 보다 세밀하게 잘 찾아내는 모습을 보인다. 다만 FCN이 잘 맞추는 곳에서 실수를 할 때도 있는데, 결국 둘을 앙상블하여 conditional random field로 후처리하면 두 가지 모델을 모두 뛰어넘게 되어, FCN과 상호 보완적인 관계에 있다고 논문은 맺는다.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://modulabs-biomedical.github.io/Learning_Deconvolution_Network_for_Semantic_Segmentation&quot;&gt;Learning Deconvolution Network for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>06: Fully Convolutional Networks</title>
        <link>/ds/dl/2020/09/07/computer-vision-06-Fully-Convolutional-Networks.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/07/computer-vision-06-Fully-Convolutional-Networks.html</guid>
        <description>&lt;h2 id=&quot;task&quot;&gt;Task&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Segmentation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbLRsBh%2FbtqvFWzSkG4%2FzVFXbcqvOHsbDeQ9NlkTOK%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Types of Task&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;semantic segmentation은 이미지 내에 있는 물체들을 의미 있는 단위로 분할해내는 것이다. 좀 더 구체적으로 이야기하면, 이미지의 각 픽셀이 어느 클래스에 속하는지 예측하는 것이다. 이렇게 이미지 내 모든 픽셀에 대해서 예측을 진행하기 때문에 이 과제를 dense prediction이라고 부르기도 한다. 어떤 이미지 내에는 사람, 자동차, 강아지, 고양이, 노트북, 선풍기 등 여러 종류의 물체가 포함되어 있을 수 있다. 이렇게 서로 다른 종류의 물체들을 깔끔하게 분할해내는 것이 semantic segmentation의 목적이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr045deeplabsemanticsegmentation-171105113047/95/pr045-deep-labsemanticsegmentation-6-638.jpg?cb=1509881698&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;관련 논문들 &lt;del&gt;할거개많다&lt;/del&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이 중에서 의미론적인 구분을 하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sementic Segmentation&lt;/code&gt; 중에서 Deep Lab에 대해 알아볼 것이다. 다음으로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U-Net&lt;/code&gt;, 마지막으로 instance 단위로 segmentation을 진행하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instance segmentation&lt;/code&gt;을 알아볼 것이다.&lt;/p&gt;

&lt;h2 id=&quot;알아두어야-할-것&quot;&gt;알아두어야 할 것&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;평가 metric
    &lt;ul&gt;
      &lt;li&gt;IoU(intersection over Union)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;사용 데이터셋
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://pjreddie.com/projects/pascal-voc-dataset-mirror/&quot;&gt;Pascal VOC 2012&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pjreddie.com/projects/pascal-voc-dataset-mirror/&quot;&gt;cityscape&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;fully-convolutional-networks&quot;&gt;Fully Convolutional Networks&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Fully connected 대신 Fully convolution!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Baseline
    &lt;ul&gt;
      &lt;li&gt;Fully Convolutional Networks for Semantic Segmentation”, 2014&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sematic Segmentation = Pixel level Calssification??&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CNN은 image classification을 잘한다. 그렇다면 Sememtation은 pixel 단위로 classification을 진행하면 되지 않을까? 하지만 굉장히 비효율적이다. 그렇다면, 최대한 기존 network의 구조를 변경하지 않으면서 이걸 가능하게 할 방법은?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr045deeplabsemanticsegmentation-171105113047/95/pr045-deep-labsemanticsegmentation-17-638.jpg?cb=1509881698&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdYXtID%2FbtqvFhyuhNf%2FTf0TfXSNpXZmdbmQMzQRiK%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;기존 VGG16 network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;AlexNet, VGGNet 등 이미지 분류(image classification)용 CNN 알고리즘들은 일반적으로 컨볼루션 층들과 fully connected 층들로 이뤄져있다. 입력이미지에 의존도가 크기 때문에 항상 입력이미지를 네트워크에 맞는 고정된 사이즈로 작게 만들어서 입력해준다. 그러면 네트워크는 그 이미지가 속할 클래스를 예측해서 알려준다. 아래 그림에서 네트워크는 입력된 이미지의 클래스를 얼룩무늬 고양이(tabby cat)라고 예측해냈다.&lt;/p&gt;

&lt;p&gt;이 분류용 CNN 알고리즘들은 이미지에 있는 물체가 어떤 클래스에 속하는지는 예측해낼 수 있지만, 그 물체가 어디에 존재하는지는 예측해낼 수 없다. 왜냐하면 네트워크 후반부의 fully connected 층에 들어서면서 위치정보가 소실되었기 때문이다. 따라서 AlexNet, VGGNet 등과 같은 알고리즘들을 수정함없이 Semantic segmentation 과제에 그대로 사용하는 것은 불가능하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr045deeplabsemanticsegmentation-171105113047/95/pr045-deep-labsemanticsegmentation-19-638.jpg?cb=1509881698&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcSbIVE%2FbtqvI5YqCii%2Fk6AP2w27BjDtdEQYKgu5Tk%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Fully Convolutional Network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그래서 저자들은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1x1 filter&lt;/code&gt;를 사용하여 위치정보를 살렸다. 마지막 단에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;7x7 heatmap&lt;/code&gt; 을 output으로 내고, 이를 원래 image size로 upsampling을 진행하여 문제를 해결하고자 하였다.&lt;/p&gt;

&lt;p&gt;결과적으로 네트워크 전체가 컨볼루션층들로 이뤄지게 되었다. fully connected 층들이 없어졌으므로 더 이상 입력 이미지의 크기에 제한을 받지 않게 되었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr045deeplabsemanticsegmentation-171105113047/95/pr045-deep-labsemanticsegmentation-20-638.jpg?cb=1509881698&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbFIfAW%2FbtqvHhd0D3s%2FFIyW7ZKEzL1DFOjk0EfFA1%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;입력 크기에 대해 자유로워졌다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;저자들은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fully Convolutional Network&lt;/code&gt;는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1x1 filter&lt;/code&gt;를 사용하기 때문에 input image에 대한 dependency가 작은 것도 장점이라 했다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1x1 conv&lt;/code&gt;는 Width, Height에 대해 의존도가 없기 때문에 channel 수를 마음대로 조정하여 원하는 output 모양을 맞출 수 있다. 여러 층의 컨볼루션층들을 거치고 나면 특성맵(feature map)의 크기가 H/32 x W/32가 되는데, 그 특성맵의 한 픽셀이 입력이미지의 32 x 32 크기를 대표한다. 즉, 입력이미지의 위치 정보를 ‘대략적으로’ 유지하고 있는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;upsampling&quot;&gt;Upsampling&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;feature map을 키워서 image size로 만들자!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc2rRKu%2FbtqvFhluGR4%2FL0t8Da4wcUNJzkxPmzH6tk%2Fimg.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Heatmap과 Upsampling&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;입력이미지의 위치 정보를 가지고 있는 Heapmap은 아직 대략적인 정보(Coarse)일 뿐이다. 이러한 정보를 기반으로 원래 이미지의 Pixel Size에서 class를 예측하는 dense Prediction을 수행해야 한다. 이 과정에서 Heat map을 원래 image size로 크기를 키워주는 과정을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsampling&lt;/code&gt;이라 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbwdTpY%2FbtqvHixmi52%2FBq3qFblKq2M59qH3DTQ6Xk%2Fimg.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;FCN-32s 모델의 전체 과정&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;하지만 단순히 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsampling&lt;/code&gt;을 진행하면 예상하겠지만 여전히 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Coarse&lt;/code&gt;한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;segmentation map&lt;/code&gt;을 얻게 된다. 단숨에 32배 한다면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;coarse&lt;/code&gt; 할수 밖에 없다. 이렇게 단숨에 32배 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsampling&lt;/code&gt;한 네트워크를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FCN-32s&lt;/code&gt;라 소개하고 있다. 확실히 정확도가 많이 떨어진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc8w6Q4%2FbtqvIl8hnlM%2FluT2LiEWwy1s6C6iBLOB9K%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;60%&quot; /&gt;&lt;em&gt;FCN-32s : Coarse Output&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;skip-combining&quot;&gt;Skip Combining&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;이전 단계의 컨볼루션층들의 특성맵을 참고하여 upsampling을 해주자!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FIGdNu%2FbtqvIlt4uDG%2FfcPrxA9rRuGSK0k7urG1SK%2Fimg.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;FCN-16s&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;컨볼루션과 풀링 단계로 이뤄진 이전 단계의 컨볼루션층들의 특성맵을 참고하여 upsampling을 해주면 좀 더 정확도를 높일 수 있지 않겠냐는 생각에서 Skip combining이라는 방법을 제안한다. 왜냐하면 이전 컨볼루션층들의 특성맵들이 해상도 면에서는 더 낫기 때문이다. 이렇게 바로 전 컨볼루션층의 특성맵(pool4)과 현재 층의 특성맵(conv7)을 2배 upsampling한 것을 더한다. 그 다음 그것(pool + 2x conv7)을 16배 upsampling으로 얻은 특성맵들로 segmentation map을 얻는 방법을 FCN-16s라고 부른다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcUvGlz%2FbtqvJ6CMaea%2FmhXVZg7xJk9rEdLR7KRgWk%2Fimg.jpg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;FCN-8s&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;또 더 나아가서 전전 컨볼루션층의 결과도 참고해서 특성맵들을 얻고, 또 그 특성맵들로 segmentation map을 구할 수도 있다. 이 방법은 FCN-8s라고 부른다. 좀 더 구체적으로 이야기하면, 먼저 전전 단계의 특성맵(pool3)과 전 단계의 특성맵(pool4)을 2배 upsampling한 것과 현 단계의 특성맵(conv7)을 4배 upsampling 한 것을 모두 더한 다음에 8배 upsampling을 수행하므로 특성맵들을 얻는다. 이것을 모두 종합해서 최종 segmentation map을 산출한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FpU9Xh%2FbtqvGCXt7hJ%2FyFa9DNVZi99eGvVoBXut8k%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;skip combining의 depth에 따른 결과 비교&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://bskyvision.com/491&quot;&gt;semantic segmentation의 목적과 대표 알고리즘 FCN의 원리&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>05: Faster R-CNN</title>
        <link>/ds/dl/2020/09/02/computer-vision-05-Faster-RCNN.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/02/computer-vision-05-Faster-RCNN.html</guid>
        <description>&lt;h2 id=&quot;핵심-아이디어&quot;&gt;핵심 아이디어&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Region Proposal도 Network안에 포함시키자!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Faster R-CNN의 핵심 아이디어는 Resion Proposal Network(이하 RPN)이다. 기존 Fast R-CNN구조를 계승하면서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;selective search&lt;/code&gt;를 제거하고 RPN을 통해서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Roi&lt;/code&gt;를 계산한다. 이를 통해서 GPU를 통해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Roi&lt;/code&gt;를 계산할 수 있게 되었고, 이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoI&lt;/code&gt;를 추출하는 것 역시 학습시켜 정확도를 높일 수 있다. 결과적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;selective search&lt;/code&gt;가 2000개 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoI&lt;/code&gt;를 계산하는데 반해, 800개 정도로 더 높은 정확도를 가진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbUjRYz%2FbtqAWb0p8cv%2Fdx8Ky33sdZtb2RKQ8sQxZK%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;60%&quot; /&gt;&lt;em&gt;Faster R-CNN structure&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그림을 보면 알겠지만, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature map&lt;/code&gt;으로 부터 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;selective search&lt;/code&gt;를 거치치 않고 이를 RPN에 전달하여 계산을 진행한다. 여기서 얻은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoI&lt;/code&gt;로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoI Pooling&lt;/code&gt;을 진행한 다음 object detection을 진행한다.&lt;/p&gt;

&lt;h1 id=&quot;region-proposal-network&quot;&gt;Region Proposal Network&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fo7PTm%2FbtqAXir1rPy%2FVbzsfY9JMY9N3ixCe3zxb0%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;60%&quot; /&gt;&lt;em&gt;Region Proposal Network structure&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이 그림보다는 순차적으로 된 그림으로 이해하는 것이 쉽다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb7xNNb%2FbtqAYHyrFDU%2FJDkko5dBYTMzZV96AcpakK%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Region Proposal Network structure&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;CNN을 통해 뽑아낸 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature map&lt;/code&gt;을 입력으로 받는다. 어떤 pretrained model을 사용할 지 모르므로 이를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HxWxC&lt;/code&gt;로 둔다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature map&lt;/code&gt;에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(3x3)x256&lt;/code&gt; 또는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(3x3)x512&lt;/code&gt; conv 연산을 수행한다. 엄밀히 말하면 C와 256, 512는 같아야 한다. 일단 연산이 가능하다고 가정하자. 이 때, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HxW&lt;/code&gt;가 보존될 수 있게 padding을 1로 설정한다.&lt;/p&gt;

&lt;p&gt;전 과정에서 나온 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature map&lt;/code&gt;을 가지고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt;을 위한 확률값과, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bounding box regression&lt;/code&gt; 값을 뽑아낸다. 이 과정에서 너무 많은 연산을 진행하게 되면 모델이 지나치게 무거워 진다. 저자들은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1 x 1 conv&lt;/code&gt;만을 수행하여 예측값을 뽑아내고자 하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/37871541/92203883-385bee80-eebd-11ea-9782-e8e3c0e5c95d.png&quot; alt=&quot;image&quot; class=&quot;center&quot; width=&quot;60%&quot; /&gt;&lt;em&gt;Anchor&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;먼저 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Classification&lt;/code&gt;의 경우, 더욱 가볍게 진행하기 위해 물체인지 아닌지를 구분하는 binary &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt;을 진행하고자 하였다. 하지만 이 문제는 bounding box와 엮어서 이를 생각해야 하는데, 저자들은 이 단계에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Anchor&lt;/code&gt;라는 개념을 도입하여 이를 진행하였다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Anchor&lt;/code&gt;는 간단하게 사전에 정의해 둔 Box들이다. 총 9개를 사용하였다.&lt;/p&gt;

&lt;p&gt;이 모든 내용을 정리하면, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt;의 결과는 총 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(HxW)&lt;/code&gt;의 각각의 위치에 제안된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Anchor&lt;/code&gt;(9개)에 대해 물체의 여부(2)를 나타내는 총 18개의 Node를 가져야 한다. 그러기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(1x1)x(2x9)&lt;/code&gt;의 conv 연산을 진행하였다. 결과적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(HxW)x(2x9)&lt;/code&gt;의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Feature map&lt;/code&gt;이 나오고, 각각의 노드는 순서대로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(h, w)&lt;/code&gt; 위치에 있는 1번 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor&lt;/code&gt;가 물체일 logit, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(h, w)&lt;/code&gt; 위치에 있는 1번 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor&lt;/code&gt;가 물체가 아닐 logit … 로 정의된다. 최종적으로 이를 확률 값으로 변경해주기 위해 적절히 reshape 해준 다음 Softmax를 적용한다.&lt;/p&gt;

&lt;p&gt;두번째로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bounding Box Regression&lt;/code&gt;을 진행한다. 같은 방법을 사용한다. 이번에는 9개 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor&lt;/code&gt;에 대해 총 4개의 좌표를 수정하기 위한 조절값을 예측해야 하므로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(H W)x(4x9)&lt;/code&gt;의 결과를 얻어야 한다. 이번에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;regression&lt;/code&gt;이기 때문에 그대로 결과값으로 사용하면 된다.&lt;/p&gt;

&lt;p&gt;앞선 과정은 순차적으로 진행된다. 즉, classification을 먼저 진행하고, 이 결과를 기반으로 물체일 확률을 sorting한다. 이 중 높은 순으로 K개의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor&lt;/code&gt;를 후보군으로 선정한다. 이 후보군에 각각 bounding Box &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Regression&lt;/code&gt;을 진행한다. 마지막으로 Non-Maximum-Suppression을 적용하고, 이것을 기반으로 RoI를 제안한다.&lt;/p&gt;

&lt;p&gt;이러한 방법을 통해서 RoI를 제안하는 Network를 만들었다. 이 후 과정은, 이렇게 만들어진 RoI를 첫번째 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Feature map&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(HxWxC) &lt;/code&gt;에 투영하는 과정을 거친다. 이 부분은 &lt;a href=&quot;https://wansook0316.github.io/ds/dl/2020/09/02/computer-vision-04-Fast-RCNN.html&quot;&gt;Fast R-CNN&lt;/a&gt; 구조와 같다.&lt;/p&gt;

&lt;h3 id=&quot;rpns-loss-function&quot;&gt;RPN’s Loss function&lt;/h3&gt;

&lt;p&gt;RPN은 앞서서 Classification과 Bouding Box Regression을 수행했다. 로스 펑션은 이 두 가지 테스크에서 얻은 로스를 엮은 형태를 취하고 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\{p_i\}, \{t_i\})= {1 \over N_{cls}}\sum_i L_{cls}(p_i, p_i^*) + \lambda {1 \over N_{reg}}\sum_i p_i^* L_{reg}(t_i, t_i^*)&lt;/script&gt;

&lt;p&gt;여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;는 하나의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor&lt;/code&gt;를 말한다. $p_i$는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt;을 통해서 얻은 해당 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anchor&lt;/code&gt;가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;object&lt;/code&gt;일 확률을 의미한다. $t_i$는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bounding box regression&lt;/code&gt;을 통해서 얻은 박스 조정 값 벡터를 의미한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;이 붙은 변수는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ground truth label&lt;/code&gt;에 해당된다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classification&lt;/code&gt;은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary cross entropy&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;regression&lt;/code&gt;은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;smooth L1 loss&lt;/code&gt;를 사용한다.&lt;/p&gt;

&lt;p&gt;주목해야 할 점은 각각 $N_{cls}$와 $N_{reg}$를 가진다는 점이다. $N_{cls}$는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;minibatch&lt;/code&gt; 사이즈이며 논문에서는 256입니다. $N_{reg}$는 엥커 개수에 해당하며 약 2400개 (256 x 9)에 해당한다. 실제 실험을 진행했을 떄 이부분이 큰 부분을 담당하지는 않는다고 말한다. $\lambda$는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Classifiaction Loss&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Regression Loss&lt;/code&gt; 사이에 가중치를 조절해주는 부분인데 논문에서는 10으로 설정되어 있어, 사실상 두 로스는 동일하게 가중치가 매겨진다. 이후는 &lt;a href=&quot;https://wansook0316.github.io/ds/dl/2020/09/02/computer-vision-04-Fast-RCNN.html&quot;&gt;Fast R-CNN&lt;/a&gt; 구조와 같다. 이제 남은 것은 어떻게 이 두 네트워크를 학습시키느냐에 대한 것이다.&lt;/p&gt;

&lt;h2 id=&quot;training-method&quot;&gt;Training Method&lt;/h2&gt;

&lt;p&gt;하지만 전체 모델을 한번에 학습시키기란 매우 어려운 작업이다. RPN이 제대로 RoI를 계산해내지 못하는데 뒷 단의 Classification 레이어가 학습될 리가 없다. 여기서 저자들은 4단계에 걸쳐서 모델을 번갈아서 학습시키는 Alternating Training 기법을 취한다. 말이 어렵지 그냥 따로 하고 지지고 볶으면서 학습시킨거다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ImageNet pretrained&lt;/code&gt; 모델을 불러온 다음, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RPN&lt;/code&gt;을 학습시킨다.&lt;/li&gt;
  &lt;li&gt;1 단계에서 학습시킨 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RPN&lt;/code&gt;에서 기본 CNN을 제외한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Region Proposal 레이어&lt;/code&gt;만 가져온다. 이를 활용하여 Fast RCNN을 학습시킨다. 이 때 , 처음 피쳐맵을 추출하는 CNN까지 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine tune&lt;/code&gt; 시킨다.&lt;/li&gt;
  &lt;li&gt;앞서 학습시킨 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fast RCNN&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RPN&lt;/code&gt;을 불러온 다음, 다른 웨이트들은 고정하고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RPN&lt;/code&gt;에 해당하는 레이어들만 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine tune&lt;/code&gt; 시킨다. 여기서부터 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RPN&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fast RCNN&lt;/code&gt;이 컨볼루션 웨이트를 공유하게 된다.&lt;/li&gt;
  &lt;li&gt;마지막으로 공유하는 CNN과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RPN&lt;/code&gt;은 고정시킨 채, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Fast R-CNN&lt;/code&gt;에 해당하는 레이어만 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine tune&lt;/code&gt; 시킨다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;의의&quot;&gt;의의&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;region proposal&lt;/code&gt;을 한번에 수행&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;한계&quot;&gt;한계&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;여전히 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;real time&lt;/code&gt;이라고 하기에는 무리가 있음&lt;/li&gt;
  &lt;li&gt;여전히 학습과정이 복잡하고 2step 임&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://yeomko.tistory.com/17?category=888201&quot;&gt;갈아먹는 Object Detection [4] Faster R-CNN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>04: Fast R-CNN</title>
        <link>/ds/dl/2020/09/02/computer-vision-04-Fast-RCNN.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/02/computer-vision-04-Fast-RCNN.html</guid>
        <description>&lt;h2 id=&quot;핵심-아이디어&quot;&gt;핵심 아이디어&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Feature Extraction, classification, bounding box regression까지 한번에 학습할 수 있는 모델을 만들자!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Fast R-CNN은 이전 SSP Net이 가지는 한계점을 극복하는 시도에서 출발한다. SSP Net은 1) Multi stage model이고 2) FC layer 만 학습 시킬 수 있다는 한계점이 있었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://media.geeksforgeeks.org/wp-content/uploads/20200219160147/fast-RCNN1.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;Fast R-CNN Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;알고리즘&quot;&gt;알고리즘&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;pretrained model로 부터 feature map을 추출한다.&lt;/li&gt;
  &lt;li&gt;Selective Search를 통해 찾은 각각의 ROI에 대해 &lt;strong&gt;*ROI Pooling&lt;/strong&gt;을 진행한다. 그 결과로 고정된 크기의 feature vector를 얻는다.&lt;/li&gt;
  &lt;li&gt;feature vector는 FC layer를 통과하고 두개의 branch로 나뉜다.&lt;/li&gt;
  &lt;li&gt;하나의 branch에서는 softmax를 통과하여 해당 ROI가 어떤 물체인지 clasification을 진행한다.&lt;/li&gt;
  &lt;li&gt;다른 branch에서는 bounding box regression을 통해 selective search로 찾은 박스의 위치를 조정한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;핵심 의의는 multi stage model에서 end-to-end로 model을 구성했다는 것에 있다. 결과적으로도 속도, 정확도, 학습 속도 모두를 향상시켰다는데 의의가 있다.&lt;/p&gt;

&lt;h2 id=&quot;roi-polling&quot;&gt;ROI polling&lt;/h2&gt;

&lt;p&gt;Roi pooling의 아이디어는 앞서 보았던 SPP Net과 유사하다. SPP Net은, pretrained model으로 부터 도출되는 feature map으로 부터, 피라미드 filter를 거친 후 이를 vectorize 하여 고정된 개수의 vector를 얻을 수 있었다. 이 아이디어를 조금 변경하여 제시하는 것이 Roi pooling이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmF4V0%2FbtqAVGST2nx%2FNhjfsG6vd89TgIK5bn2Ha0%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;ROI pooling&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;feature map에서 Selective search를 통해 Resion Proposal을 진행한다.&lt;/li&gt;
  &lt;li&gt;이 proposal에 Roi pooling을 진행하여 고정된 형태의 작은 feature map을 만든다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Roi pooling은, Resion Proposal을 고정된 형태의 output 모양으로 바꾼다. (H x W) 크기의 feature map을 output으로 원한다면, proposal을 이에 맞게 칸을 나눈 후, max pooling을 진행한다. 이렇게 되면 항상 같은 크기의 결과를 얻을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;multi-task-loss&quot;&gt;Multi Task Loss&lt;/h2&gt;

&lt;p&gt;딥러닝을 공부하면서 가장 새롭고 즐거웠던 부분은 손실함수 부분이었다. object detection은 기본적으로 bounding box regression과 classication을 동시에 진행해야 하는 Task이다. 그래서 예전 접근은 multi stage로 이루어졌었다. 하지만 이 Fast R-CNN에서 처음으로 이 두가지 task를 하나로 엮는 방법이 고안된다.&lt;/p&gt;

&lt;p&gt;우리는 이미지로 부터 feature map을 추출했고, 이 feature map에서 Roi를 제안 받아 Roi pooling을 통해 feature vector를 만들었다. 이제 이 벡터로 classification과 bounding box regression을 적용하여 각각의 loss를 얻어내고, 이를 back propagation하여 전체 모델을 학습시키면 된다. 이 두 Task 모두를 반영한 손실함수를 보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(p, u, t^u, v) = L_{cls}(p, u) + \lambda[u \ge 1]L_{loc}(t^u, v)&lt;/script&gt;

&lt;p&gt;각 변수 하나하나에 대해서 알아보자. 먼저, $p$ 는, Softmax를 통해 얻어낸 $K+1$ 개의 확률값이다.(이산 확률 분포) $K+1$인 이유는 K개의 object와 배경(아무 물체도 아님)을 추가한 것이다. $u$는 해당 Roi의 ground truth label 벡터이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p = (p_0, ..., p_n,..., p_k)\\
u = (0, ..., 1, ..., 0)&lt;/script&gt;

&lt;p&gt;다음으로는 &lt;a href=&quot;https://wansook0316.github.io/ds/dl/2020/09/02/PaperReview-01-RCNN.html&quot;&gt;bounding box regression&lt;/a&gt;을 진행한다. 고정 처리된 feature map을 가지고 regression을 했을 때 결과는, 각각의 class (K + 1) 에 대해 각각 x, y, w, h를 조정하는 파라미터 $t^k$를 리턴한다. 말로 풀어보면 다음과 같다. feature map으로 부터 1번 클래스 일 때 (x, y, w, h)를 ($t_x$, $t_y$, $t_w$, $t_h$) 로 변화시켜. 2번 클래스 일때는 …(중략). 이 중에서 우리가 하고 싶은 것은, 이 결과를 바탕으로 이를 수정하는 loss function을 만들고 싶은 것이므로 이 결과들 중 ground truth에 속하는 u번째 t만 가져와서 사용한다. $v$는 ground truth bounding box 조절 값에 해당한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t^k = (t^k_x, t^k_y, t^k_w, t^k_h)\\
t^u = (t^u_x, t^u_y, t^u_w, t^u_h)\\&lt;/script&gt;

&lt;p&gt;그렇다면 이제 각각의 loss function에 대해서 알아보자. 먼저 classification loss 는 log loss를 사용한다. 못맞출 수록 패널티를 크게 준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{cls}(p, u) = -logp_u&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;location&lt;/code&gt;을 담당하는 loss는 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_{loc}(t^u, v) = \sum_{i \in {x, y, w, h}}smooth_{L_1}(t^u_i - v_i)&lt;/script&gt;

&lt;p&gt;bounding box를 만들기 위한 예측 조절값에서 실제 조절값을 smooth L1을 통과시킨 것의 합을 사용한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
smooth_{L_1}(x) =

\begin{cases}
0.5x^2 &amp; \mbox {if }\left| x \right| &lt; 1 \mbox{ is even} \\
\left| x \right|-0.5 &amp; otherwise
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;저자들은 실험 과정에서 라벨 값과 지나치게 차이가 많이 나는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outlier&lt;/code&gt;가 많았고, 이런 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outlier&lt;/code&gt;에 민감하게 반응하는 L2 loss를 그대로 사용할 경우 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gradient explode&lt;/code&gt;현상이 발생하는 것을 확인했다고 한다. 이를 제어하기 위해 custom한 loss function을 사용했다.&lt;/p&gt;

&lt;h2 id=&quot;backpropagation-through-roi-pooling-layer&quot;&gt;Backpropagation through RoI Pooling Layer&lt;/h2&gt;

&lt;p&gt;이제 네트워크를 학습하면 된다. 그런데 이전의 SSP Net을 보면, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature map&lt;/code&gt;을 뽑아낸 후, SSP를 거쳐 나온 vector들에 대해 FC layer를 구성하고, 이 단계만 학습시켰던 것을 기억할 거다.(fine tuning) 위 논문에서 저자들은, 이미지의 특징을 추출하는 가장 중요한 역할인 CNN이 학습될 수 없다는 것에 집중한다. 즉, 어느 단계까지 fine tuning을 진행할 것인지, 또 그 fine funing을 진행할 경우 학습이 진행이 되는지(역전파가 전달이 되는지)를 이론적으로 검증한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\partial L \over \partial x_i } = \sum_r \sum_j [i = i^*(r, j)]{\partial L \over \partial y_{rj} }&lt;/script&gt;

&lt;p&gt;$x_i$라고 하는 것은 CNN을 통해 추출된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature map&lt;/code&gt;에서 하나의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature&lt;/code&gt;를 의미하고 이는 실수이다. 전체 Loss에 대해서 이 피쳐 값의 편미분 값을 구하면 그 값이 곧 xi에 대한 loss 값이 되며 역전파 알고리즘을 수행할 수 있다. 이제 피쳐 맵에서 RoI를 찾고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoI Pooling&lt;/code&gt;을 적용하기 위해서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H x W&lt;/code&gt; 크기의 grid로 나눈다. 이 그리드들을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-window&lt;/code&gt;라 부르며, 위 수식에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt;란 몇번째 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-window&lt;/code&gt;인지를 나타내는 인덱스이다. $y_{rj}$는 이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Roi Pooling&lt;/code&gt; 을 통과하여 최종적으로 얻어진 ouput의 값이며 이 역시 실수이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcHlzBy%2FbtqAThsNYhK%2FVKc46d2mKurHG7foMCN2wk%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;60%&quot; /&gt;&lt;em&gt;Back Propagation through RoI Pooling&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$x_i$ 가 최종 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prediction&lt;/code&gt; 값에 영향을 주려면 $x_i$가 속하는 모든 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Roi&lt;/code&gt;의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-window&lt;/code&gt; 에서 해당 $x_i$가 최댓값이 되야 한다. $i^*(r, j)$란 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Roi&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-window index j&lt;/code&gt;가 주어졌을 때, 최대 피쳐 값의 인덱스를 말한다.&lt;/p&gt;

&lt;p&gt;즉 수식을 보면 $[i = i^*(r, j)]$ 이렇게 표현되어 있는데, 최대 패쳐 인덱스가 내가 구하길 원하는 피쳐와 같을 때는 1을 return, 아니면 0 을 return 하라는 의미이다. 결과적으로 우리는 $\partial L \over \partial y_{rj}$ 이 값을 가지고 있고, 발생하는 모든 이 값을 더해서 적용시켜주면 $x_i$에 대한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gradient&lt;/code&gt;를 구할 수 있다.&lt;/p&gt;

&lt;p&gt;종합하면, 우리는 앞서 구한 multitask loss를 RoI Pooling layer를 통과하여 CNN 단까지 fine-tuning 할 수 있다. 저자드은 실험을 통해서 실제로 CNN단 까지 fine tuning 하는 것이 성능 향상에 도움이 되었다는 실험 결과를 보여준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbadZIp%2FbtqAVIwqRP6%2FW9hTlTIcKm6JNlFDTsWf4K%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;60%&quot; /&gt;&lt;em&gt;fine tuning depth에 따른 성능 변화&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;위 실험 결과는 fine-tuning 하는 깊이를 조절해가며 성능 변화를 실험한 것이다. CNN의 단을 깊이 학습시킬 수록 성능이 향상되었으며, 이 때 테스트에 소요되는 시간 변화는 거의 없는 것을 확인할 수 있다. 즉, CNN 단을 Object Detection에 맞게끔 fine-tuning 하는 것이 성능 향상의 키 포인트였다.&lt;/p&gt;

&lt;h2 id=&quot;의의&quot;&gt;의의&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;end-to-end&lt;/code&gt; 모델 제안&lt;/li&gt;
  &lt;li&gt;학습 단계 간소화&lt;/li&gt;
  &lt;li&gt;정확도, 성능 개선&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;한계&quot;&gt;한계&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;region proposal을 selective search를 사용
    &lt;ul&gt;
      &lt;li&gt;이는 CPU 연산으로만 가능하기 때문에 병목이 발생&lt;/li&gt;
      &lt;li&gt;이 부분이 inference를 수행하는데 있어 가장 많은 시간을 차지함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://yeomko.tistory.com/15?category=888201&quot;&gt;갈아먹는 Object Detection [3] Fast R-CNN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>03: Spatial Pyramid Pooling Network</title>
        <link>/ds/dl/2020/09/02/computer-vision-03-Spatial-Pyramid-Pooling-Network.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/02/computer-vision-03-Spatial-Pyramid-Pooling-Network.html</guid>
        <description>&lt;h2 id=&quot;핵심-아이디어&quot;&gt;핵심 아이디어&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;입력 이미지의 크기나 비율에 관계없이 CNN 학습은 불가한가?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Fast R-CNN으로 넘어가기전 상당히 많은 아이디어를 가져온 논문이다. 이전의 R-CNN을 보게되면, proposal roi가 CNN에 들어가기 전에 입력 이미지를 바꿔주어야 하는 한계가 존재했다. 여기서 저자들은 의문을 갖는다. 이 제한 요소를 없앤 상태로 CNN을 학습시키는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdb1FzH%2FbtqASyVypzb%2FGpCrnYjeKY1Si6LjftCoO0%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;SPPNet의 핵심 아이디어&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;사실 CNN의 입력이미지 크기는 고정될 필요가 없다. CNN의 핵심 아이디어는 filter를 가지고 연산을 수행하는 것이고, 이것의 연산 방식은 sliding window 방식으로 진행된다. 하지만, 이 입력 이미지의 크기가 고정이어야 하는 이유는, 마지막에 도출되는 fully connected layer의 크기가 고정적으로 나와야 하기 때문이다. 이 문제점으로 부터 SPPNet가 제안된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;입력 이미지 상관 없이 통과시키고, FC 전에 polling을 통해서 동일한 크기로 만들자!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;굉장히 단순한 방법을 제안하였다. (…) 이런 방식을 사용할 경우, 원본 이미지의 특징을 고스란히 간직한 feature map을 얻을 수 있다. 추가적으로 비율도 조절하지 않기 때문에, 사물의 크기에 따른 변화도 감지가 가능하다.&lt;/p&gt;

&lt;p&gt;위의 그림을 보면 Crop 후 conv에 넣는 것이 아니고, feature map을 만든 후, 이를 SSPNet에 넣어 모양을 맞춘 후에 output을 만드는 것을 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;알고리즘&quot;&gt;알고리즘&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;전체 이미지를 pretrained model을 통과시켜 feature map을 추출한다.&lt;/li&gt;
  &lt;li&gt;해당 feature map으로 부터 selective search를 통해 ROI를 뽑아낸다. 이 때 발생하는 ROI는 모두 크기와 비율이 다르다. 여기서 &lt;strong&gt;&lt;em&gt;SSPNet&lt;/em&gt;&lt;/strong&gt;을 적용하여 고정된 크기의 feature vector를 추출한다.&lt;/li&gt;
  &lt;li&gt;FC layer를 통과시킨다.&lt;/li&gt;
  &lt;li&gt;앞서 추출한 벡터로 각 이미지 클래스 별로 SVM을 학습시킨다.&lt;/li&gt;
  &lt;li&gt;마찬가지로 해당 벡터로 bounding box regressor를 학습시킨다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;spatial-pyramid-pooling&quot;&gt;Spatial Pyramid Pooling&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&amp;amp;fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F99D6063A5C53E7F5294025&quot; alt=&quot;&quot; class=&quot;center&quot; /&gt;&lt;em&gt;출처 : http://kaiminghe.com/eccv14sppnet/index.html&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 어떤 방식으로 SPP가 돌아가는지 이해해야 한다. 먼저 CNN을 거친 feature map을 input으로 받는다. 그리고 이것을 미리 정해져 있는 영여긍로 나누어 준다. 위의 예시에서는 4x4, 2x2, 1x1 3개의 영역이 적용되어 있고, 이 각각을 하나의 피라미드라 부른다. 즉, 3개의 피라미드를 설정한 것.&lt;/p&gt;

&lt;p&gt;이 피라미드는 4x4 짜리 고정된 CNN 필터 같은 것이 아니다. 어떠한 input이 들어오더라도 4x4 격자로 만든다는 표현이 더 맞는 표현이다. 예를 들어 입력이 64 x 64 x 256 크기의 피쳐 맵이 들어온다고 했을 때, 4x4의 피라미드의 bin의 크기는 16x16이 된다.&lt;/p&gt;

&lt;p&gt;이제 이 각각의 bin에서 가장 큰 값만 추출하는 max pooling을 수행하고, 그 결과를 &lt;strong&gt;쭉 이어 붙인다&lt;/strong&gt;. 입력 feature map의 채널 크기가 k, bin의 개수를 M이라 한다면, 해당 SSP의 output은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k x M&lt;/code&gt;의 크기를 가진 1차원의 벡터가 될 것이다.&lt;/p&gt;

&lt;h2 id=&quot;한계&quot;&gt;한계&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;여전히 multi stage model이다.&lt;/li&gt;
  &lt;li&gt;여전히 SVM, selective search를 사용한다.&lt;/li&gt;
  &lt;li&gt;feature map을 만들어내는 network를 학습시키지 못한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kcPAGIgBGRs&amp;amp;list=PLWKf9beHi3Tg50UoyTe6rIm20sVQOH1br&amp;amp;index=12&quot;&gt;PR-012: Faster R-CNN : Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>02: R-CNN</title>
        <link>/ds/dl/2020/09/02/computer-vision-02-RCNN.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/02/computer-vision-02-RCNN.html</guid>
        <description>&lt;h1 id=&quot;computer-vision의-task&quot;&gt;Computer Vision의 Task&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr12fasterrcnn170528-170802143120/95/faster-rcnn-pr012-3-638.jpg?cb=1504447138&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;
이중 Object Detection에 해당하는 문제이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr12fasterrcnn170528-170802143120/95/faster-rcnn-pr012-5-638.jpg?cb=1504447138&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;속도가 느려보여도 정확도 측면에서 높은 것을 알 수 있다.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;r-cnn&quot;&gt;R-CNN&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr12fasterrcnn170528-170802143120/95/faster-rcnn-pr012-6-638.jpg?cb=1504447138&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;
&lt;img src=&quot;https://image.slidesharecdn.com/pr12fasterrcnn170528-170802143120/95/faster-rcnn-pr012-7-638.jpg?cb=1504447138&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;region proposal을 진행한다.&lt;/li&gt;
  &lt;li&gt;CNN에 각각 넣는다. -&amp;gt; &lt;strong&gt;느리다&lt;/strong&gt;, CNN을 사용하기 때문에 입력 크기가 동일해야 한다.(warpping)&lt;/li&gt;
  &lt;li&gt;CNN의 마지막 feature map에서 SVM을 사용하여 구분한다.&lt;/li&gt;
  &lt;li&gt;또한 입력으로 주어진 bounding box를 조정하기 위해 regression을 진행한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;region-proposal&quot;&gt;Region Proposal&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://image.slidesharecdn.com/pr12fasterrcnn170528-170802143120/95/faster-rcnn-pr012-8-638.jpg?cb=1504447138&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;initial bounding box를 selective search를 사용하여 뽑아낸다. -&amp;gt; 느리다.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;pretrained model = alexnet for ImageNet classification dataset
    &lt;ul&gt;
      &lt;li&gt;이미지넷에서 사전 훈련된 알렉스 넷을 사용했다. 마지막단을 잘라서 사용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;가지고 있는 데이터를 넣어서 훈련한다.&lt;/li&gt;
  &lt;li&gt;여기서 발생한 마지막 feature map을 가지고 와서 classification, bounding box regression 을 진행한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이러한 방법은, 두가지 문제를 발생시킨다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;속도&lt;/li&gt;
  &lt;li&gt;마지막 단의 feature map을 사용하기 때문에 back propagation을 통한 학습이 불가하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bounding-box-regression&quot;&gt;Bounding-Box Regression&lt;/h2&gt;

&lt;p&gt;Box는 centerX, centerY, Width, Height로 표현된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P^i = (P^i_x, P^i_y, P^i_w, P^i_h) \\
\\\\
G = (G_x, G_y, G_w, G_h)&lt;/script&gt;

&lt;p&gt;우리의 목적은 $P^i$ 박스를 최대한 G에 가깝게 이동시키는 함수를 학습시키는 것이다. 이를 표현해보면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_x(P), d_y(P), d_w(P), d_h(P)&lt;/script&gt;

&lt;p&gt;x, y의 경우는 평행이동이 연산의 전부이기 때문에 linear 연산으로 처리가 가능하다. 반면 너비와 높이는 확대, 축소 변환이 필요하다. 단순한 확대 축소 연산을 사용하게 되면, 추후에 backpropagation을 통한 학습이 어려워지기 때문에 여기서는 exp를 사용했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{G_x} = P_wd_x(P) + P_x
\hat{G_y} = P_hd_y(P) + P_y
\hat{G_w} = P_wexp(d_w(P))
\hat{G_h} = P_hexp(d_h(P))&lt;/script&gt;

&lt;p&gt;왜 굳이 식을 이렇게 만들었냐 보다는, 이러한 방식으로 제안을 하려고 했다고 생각해보자. P에 대한 변수는 초기에 제안하는 것이므로, 우리는 함수 $d_*(P)$ 가 어떤 녀석인지 아는 것이 목표이다. 그리고 이 함수를 알아내는 과정은 deep learning network를 사용하여 만들 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_*(P) =  {w}^T_*\phi_5(P)&lt;/script&gt;

&lt;p&gt;여기서 $\phi_5(P)$는 pretraioned model의 가장 마지막 feature map을 의미한다. 결국 feature맵에 선형 연산을 추가하여 원하는 함수를 구한다.&lt;/p&gt;

&lt;p&gt;그렇다면, 이제는 문제가 변화했다. ground truth에서 발생하는 함수와 제안된 방법의 함수 $w^T_* \phi_5(P)$ 의 가중치 $w^T_*$ 를 구하는 문제이다.&lt;/p&gt;

&lt;p&gt;ground truth에서 발생하는 값인 $t^i_*$는 각각의 사진 한장에 대해서 고정되어 있다. 이를 반영한 손실 함수는 다음과 같다. 저자들은 람다를 1000으로 설정하였다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_* = \underset{\hat{ w_*}}{argmin}\sum_i^N(t^i_*-{\hat{ w_*}^T}\phi_5(P))^2+\lambda \lVert {\hat{ w_*}^T} \rVert ^2&lt;/script&gt;

&lt;h2 id=&quot;한계&quot;&gt;한계&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;느리다.&lt;/li&gt;
  &lt;li&gt;SVM은 CNN을 훈련시키지 못한다.&lt;/li&gt;
  &lt;li&gt;Multostage Training Pipeline이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kcPAGIgBGRs&amp;amp;list=PLWKf9beHi3Tg50UoyTe6rIm20sVQOH1br&amp;amp;index=12&quot;&gt;PR-012: Faster R-CNN : Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>01: 컴퓨터 비전 용어 정리</title>
        <link>/ds/dl/2020/09/02/computer-vision-01-%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC.html</link>
        <guid isPermaLink="true">/ds/dl/2020/09/02/computer-vision-01-%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC.html</guid>
        <description>&lt;h2 id=&quot;descripter&quot;&gt;Descripter&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;이미지를 비교하기 위해 동일한 방법을 통해 하나의 비교 대상으로 만드는 것&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/593/1*K68boX7fmtsYmyG2LlcmhQ.jpeg&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;descripter&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;두 가지 이미지가 있다. 이 두가지 이미지가 비슷한지 아닌지를 구분하기 위해 만든 것이 descripter이다. 위의 그림에서는 픽셀의 값들을 기반으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gradients&lt;/code&gt;를 구해 이를 grid에 plot하여 표현하였다. 여기서 이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gradients&lt;/code&gt;를 descripter로 사용했다고 말한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F9LBYd%2FbtqA2VjFlEl%2FwHdGhznBKKUkKYuufpfaO1%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;HOG algorithm&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;예를 들어 HOG 알고리즘은 각 pixel에서 gradient를 구하고 이 값들을 총 8가지 방향으로 매핑한 후, 히스토그램을 생성한다. 이렇게 추출돈 Feature vector는 keypoint이고 이를 기반으로 bounding box등을 만드는데 활용한다. 요즘은 이 feature mapCNN을 통해 생성한다.&lt;/p&gt;

&lt;h2 id=&quot;region-proposal&quot;&gt;Region Proposal&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;이미지로부터 영역을 선택하기 위해 사용되는 알고리즘&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;기존의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sliding window&lt;/code&gt;방식은 매우 비효율적이었고, 이를 개선한 방법이다. “물체가 있을 법한” 영역을 빠른 속도로 찾아내는 알고리즘이다. 보편적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;selective search&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;edge box algorithm&lt;/code&gt;이 있다. 하지만 이 역시도 추후에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;end-to-end&lt;/code&gt; 방식으로 개선된다.&lt;/p&gt;

&lt;h2 id=&quot;roiregion-of-interest&quot;&gt;RoI(Region of Interest)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;이미지내에서의 관심 영역&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;원래 input에서 잘라낸 관심 영역들을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoI&lt;/code&gt;라 한다.&lt;/p&gt;

&lt;h2 id=&quot;caption-generation&quot;&gt;Caption generation&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;이미지로 부터 문장을 생성하는 것&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 연구는 Human-object interaction에 기초하여 연구되고 있다.&lt;/p&gt;

&lt;h2 id=&quot;smooth-l1-loss&quot;&gt;Smooth L1 Loss&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
smooth_{L_1}(x) =

\begin{cases}
0.5x^2 &amp; \mbox {if }\left| x \right| &lt; 1 \mbox{ is even} \\
\left| x \right|-0.5 &amp; otherwise
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;L1, L2 Loss는 생략하였다. 수식에서의 x는 $|y-\hat{y}|$로 정답 label과 차이이다. 오차가 작은 부분은 제곱을 사용했고, 그렇지 않은 부분에서는 직선을 사용했다. 이러한 방식은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L1 Loss&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L2 Loss&lt;/code&gt;의 장점을 결합한 형태이다. 즉, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;error&lt;/code&gt;가 클경우 안정적으로 loss를 감소시키고($x$)
, 작을 경우에는 L2 Loss를 사용하여 업데이트 과정중 진동을 감소시킨다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbz02fP%2FbtqA3eDjKfT%2FP8HHzmOivZTkIeGAZnPwBK%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;iou-intersection-over-union&quot;&gt;IOU (Intersection over union)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;예측한 bounding box와 ground truth box간의 겹치는 넓이 비율&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fqtl0q%2FbtqA6pYBQLE%2FjZeHHTpFgEXMkGrQCXDhqK%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;IOU&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이것은 사진으로 직관적으로 이해할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;Ablation study&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;기존 모델에서 feature를 제거하면서 영향력을 확인하는 것&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature&lt;/code&gt;는 변수보다는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;network&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;layer&lt;/code&gt;등을 말한다.&lt;/p&gt;

&lt;h2 id=&quot;jittered-examples&quot;&gt;Jittered examples&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;IoU를 기준으로 사용하겠다고 판단한 bounding box&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bounding box regression&lt;/code&gt;을 진행한 후에 각각의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proposal&lt;/code&gt;에 대해 예측한 결과 중 학습에 재사용하기 위한 샘플을 걸러낼 때 사용되는 개념이다. 예를 들어 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IoU&lt;/code&gt;가 0.5이상 인 샘플을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive sample&lt;/code&gt;이라 정의할 경우, 이 샘플을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jittered examples&lt;/code&gt;이라 한다.&lt;/p&gt;

&lt;h2 id=&quot;non-maximum-suppression-nms&quot;&gt;Non-maximum suppression (NMS)&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;동일한 클래스라 판명된 bounding box들 중 중복을 제거하는 방법&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://blog.kakaocdn.net/dn/dzskGm/btqx0sScMdc/Qs7dKbEzZIFR0U5MxzsAP0/img.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;80%&quot; /&gt;&lt;em&gt;Non-maximum suppression (NMS)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;알고리즘은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;동일한 클래스에 대해 검출된 bounding box들을 confidence 순서로 정렬한다.&lt;/li&gt;
  &lt;li&gt;가장 confidence가 높은 bounding box와 IoU가 일정 이상인 bounding box는 동일 물체를 detect했다 판단하여 지운다.
    &lt;ul&gt;
      &lt;li&gt;가장 confidence높은것만 남기고 보통 0.5이상 box들을 지운다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ohem-online-hard-example-mining&quot;&gt;OHEM (Online Hard Example Mining)&lt;/h2&gt;

&lt;p&gt;먼저, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hard Example&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Easy Example&lt;/code&gt;의 개념부터 알아보자. 사람인지 아닌지를 분류하는 모델이 있다고 하자. 우리의 목적은 이 모델을 훈련시키는 것이다. 대부분의 사람 이미지는 분류하도록 만들었다. 하지만 사람 동상과 같은 샘플에 대해서는 모델이 구분하기 어려울 것이다. 이러한 상황에서 일반적으로 잘 동작하는 샘플을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Easy Example&lt;/code&gt;, 사람 동상 이미지를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hard Example&lt;/code&gt; 이라 한다. 이런 것들을 제대로 훈련하기 위해서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hard Example&lt;/code&gt;에 대해 가중치를 주거나 해서 모델을 훈련시켜야 할 것이다.&lt;/p&gt;

&lt;p&gt;다음은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive&lt;/code&gt;와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;negative&lt;/code&gt;에 대한 개념이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive&lt;/code&gt;는 문제에서 내가 원하는 클래스를 의미한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive sample&lt;/code&gt;은 bounding box의 label이 사람인 것을 의미하고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;negative sample&lt;/code&gt;은 배경임을 의미한다.&lt;/p&gt;

&lt;p&gt;그렇다면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hard negative&lt;/code&gt;란, &lt;strong&gt;실제로는 배경인데, 사람이라고 예측한 sample&lt;/strong&gt;이다. 반대로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;easy negative&lt;/code&gt;는 실제로 배경이며 배경으로 예측했음을 의미한다.&lt;/p&gt;

&lt;p&gt;즉, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hard negative sample&lt;/code&gt;은, 네거티브 샘플이라고 보기 어렵다라는 의미이다. 해당 샘플에 대해 &lt;em&gt;배경&lt;/em&gt;이라고 말해야 하는데, confidence는 높게 나오는 상황을 말한다.&lt;/p&gt;

&lt;p&gt;우리가 알아볼 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;object detection&lt;/code&gt;문제에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resion proposal&lt;/code&gt;을 통해 여러가지 후보를 선택하게 된다. 이 후보군의 대부분은 배경이라고 말해야 하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;easy negative sample&lt;/code&gt;이 차지하고 있다. 또한 사람이라고 말해야 하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive sample&lt;/code&gt;의 개수는 매우 부족하다. 보통 이러한 상황에서는 모집단의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;balance&lt;/code&gt;를 맞추는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resampling&lt;/code&gt;을 진행하거나, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boosting&lt;/code&gt;알고리즘으로 진행하게 된다. 하지만 이것은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;label&lt;/code&gt;의 불균형을 알고있고, 이를 처리할 수 있을 때 가능하다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;detection&lt;/code&gt;문제는 후보군의 label을 모르기 때문에 이 방법은 사용할 수 없다. 그렇다면 만약 이 상황에서 그대로 훈련을 진행하게 되면, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;easy negative sample&lt;/code&gt;의 양이 너무 많기 때문에 &lt;strong&gt;&lt;em&gt;배경을 배경이라 하는 예측&lt;/em&gt;&lt;/strong&gt;만이 대다수를 이루고, 이에 대해서만 학습을 진행하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbF24Qv%2FbtqB1HkB3mV%2FUSiGT6rJJmblKDwhxYGVV0%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;Cross Entropy &amp;amp; Binary Cross Entropy&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;일반적으로 Classification에서 사용하는 Loss 함수는 Cross Entropy 이다. 이러한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imbalance&lt;/code&gt;를 고려하여 업데이트를 하지 않기 때문에, 기존의 방식을 사용할 경우, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;background&lt;/code&gt;만 잘 맞추는 요상한 모델이 결과로 도출된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbT09uA%2FbtqCPAKK5gc%2FI0CM7QEhghXMx9kbptJbJk%2Fimg.png&quot; alt=&quot;&quot; class=&quot;center&quot; width=&quot;100%&quot; /&gt;&lt;em&gt;OHEM (Online Hard Example Mining)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;우리는 결과적으로 배경은 배경이라하고, 사람은 사람이라고 하는 좋은 모델을 제작해야 한다. 그러기 위해서는 후보군의 대부분을 차지하고 있는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;easy negative sample&lt;/code&gt;에 대해서 업데이트는 줄이고, 배경인데 배경이 아니라고 하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hard negative sample&lt;/code&gt;에 대해 주된 업데이트를 진행해야 한다. 이를 위해 제안된 방법이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OHEM (Online Hard Example Mining)&lt;/code&gt;이다. 결과적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positive sample&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hard negative sample&lt;/code&gt;을 가지고 문제를 해결한다. 이와 같은 불균형 문제를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Class Imbalance&lt;/code&gt;라 한다.&lt;/p&gt;

&lt;h2 id=&quot;focal-loss&quot;&gt;Focal Loss&lt;/h2&gt;

&lt;p&gt;위의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OHEM&lt;/code&gt;과 비슷하게 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;class Imbalance&lt;/code&gt;를 해결하기 위한 방법이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss function&lt;/code&gt;을 수정하여 이를 해결한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p_t =

\begin{cases}
p &amp; \mbox {if } y = 1 \\
1-p &amp; otherwise
\end{cases}
\\

FL(p_t) = -(1-p_t)^\gamma log(p_t) %]]&gt;&lt;/script&gt;

&lt;p&gt;이런 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss funtion&lt;/code&gt;을 이해하는 가장 좋은 방법은 양 극단치를 넣어보는 것이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y=1&lt;/code&gt;일 경우, ground truth가 사람인 경우에는 해당 class가 나올 확률을 그대로 넣어준다. 즉 $p_t = p$ 이다. 그렇다면 만약 잘 맞췄을 경우에는 loss가 0에 가까워진다. 결과적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postitive&lt;/code&gt;에 대해 잘 예측할 경우 loss를 작게 주고, 그렇지 않은 경우 loss를 크게 준다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y!=1&lt;/code&gt;인 경우, $p_t = 1-p$이고, 그렇게 될 경우 $FL(p_t) = -p\gamma log(1-p)$이다. cross entropy식에서 앞항과 뒤 항의 변형을 통해 log함수가 가지는 특징을 사용했다. 잘 예측할 경우 loss를 크게 주고, 그렇지 않을 경우 loss를 작게준다. 다만 log 함수에 엮여 있는 부분은 잘 예측했을 경우에 더 큰 loss값을 주게 되므로, 이 식의 의도는, 너무 잘 예측하는 데이터(p가 계속 너무 높게 나옴)의 영향력을 줄이기 위한 것이 강하다. 실제로 OHEM 보다 성능이 더 좋다고 한다.&lt;/p&gt;

&lt;h2 id=&quot;contextual-feature&quot;&gt;Contextual feature&lt;/h2&gt;

&lt;p&gt;2d image에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;contextual based classification&lt;/code&gt;은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pixel&lt;/code&gt;의 주변 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neighborhood&lt;/code&gt; 과의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relationship&lt;/code&gt;에 초점을 맞춘 approach를 뜻한다. 즉, 어떤 특정 pixel의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;contextual feature&lt;/code&gt;는 주변 pixel들과의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relationship&lt;/code&gt;에 기반해서 추출한 feature를 뜻한다.&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ganghee-lee.tistory.com/33&quot;&gt;컴퓨터비전에서의 기본 용어 및 개념 정리&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>01: Nginx</title>
        <link>/ds/server/2020/09/01/Server-01-Nginx.html</link>
        <guid isPermaLink="true">/ds/server/2020/09/01/Server-01-Nginx.html</guid>
        <description>&lt;h1 id=&quot;nginx란&quot;&gt;nginx란?&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;트래픽이 많은 웹사이트를 위해 설계한 비동기 이벤트 기반 구조의 웹서버 소프트웨어&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;러시아의 프로그래머,이고르 시쇼브가 Apache의 C10K Problem(하나의 웹서버에 10,000개의 클라이언트의 접속을 동시에 다룰 수 있는 기술적인 문제)를 해결하기 위해 만든 Event-driven구조의 HTTP, Reverser Proxy, IMAP/POP PROXY server를 제공하는오픈소스 서버 프로그램이다.&lt;/p&gt;

&lt;h1 id=&quot;apache-vs-nginx&quot;&gt;Apache vs nginx&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Apache&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;쓰레드 / 프로세스 기반 구조로 요청 하나당 쓰레드 하나가 처리하는 구조&lt;/li&gt;
  &lt;li&gt;사용자가 많으면 많은 쓰레드 생성, 메모리 및 CPU 낭비가 심함&lt;/li&gt;
  &lt;li&gt;하나의 쓰레드 : 하나의 클라이언트 라는 구조&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;nginx&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;비동기 Event-Driven 기반 구조.&lt;/li&gt;
  &lt;li&gt;다수의 연결을 효과적으로 처리가능.&lt;/li&gt;
  &lt;li&gt;대부분의 코어 모듈이 Apache보다 적은 리소스로 더 빠르게 동작가능&lt;/li&gt;
  &lt;li&gt;더 작은 쓰레드로 클라이언트의 요청들을 처리가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;thread와-event-driven-방식&quot;&gt;thread와 Event-driven 방식&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxNzAzMjZfMTI2/MDAxNDkwNDk1NjMxNzU4.wrfzv-j7_pzF4GorDTt52dZPzLcUPwnu6JJkgvD53r0g.2xqzw_4Z557pZPaKMbg5pCF3CfvyQtpBqnZrA1p9qjYg.GIF.jhc9639/mighttpd_e01.gif.gif?type=w800&quot; alt=&quot;thread 방식&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxNzAzMjZfMTM3/MDAxNDkwNDk1NjMxNzgy.OHZ33nerX_6Hc92Mg_xjr51acwwi1P_mq3SIl7Cuhisg.niRsQQVM5CwGpXKcdOxl3bkNsmfBkqGV1ajcBpV6CvQg.GIF.jhc9639/mighttpd_e02.gif.gif?type=w800&quot; alt=&quot;Event-driven 방식&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림만 봐도 딱 알겠지만, Event-driven 방식은 java-script에서와 같이 비동기 이벤트를 처리하는 방식으로 구동된다. 그렇기 때문에 자원을 효율적으로 사용한다.&lt;/p&gt;

&lt;p&gt;그렇지 않아도 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node.js&lt;/code&gt;의 창시자 라이언 달은 nginx를 프록시 서버로 앞단에 두고, node.js를 뒤쪽에 놓는게 버퍼 오버플로우 공격을 방지할 수 있다고 하였다.&lt;/p&gt;

&lt;h2 id=&quot;버퍼-오버플로우&quot;&gt;버퍼 오버플로우&lt;/h2&gt;

&lt;p&gt;버퍼는 보통 데이타가 저장되는 메모리 공간을 뜻한다. 이 때, 메모리 공간을 벗어나는 경우 오버플로우가 되고 이 때 사용되지 않아야 할 영역에 데이터가 덮어씌워져 주소, 값을 바꾸는 공격이다.&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;h3 id=&quot;버퍼-오버플로우로-인한-큰-이슈--하트블리드사태&quot;&gt;버퍼 오버플로우로 인한 큰 이슈 : 하트블리드사태&lt;/h3&gt;

&lt;p&gt;즉, 실제포트를 숨기고 nginx의 80포트를 통해서 프록시하면 보안적으로 막을 수 있다는 것인데 이것 말고도 정적자료에 대한 gzip압축, 그리고 앞단에서의 로그를 저장할 수 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
      <item>
        <title>03: image 생성, 업로드</title>
        <link>/ds/docker/2020/09/01/Docker-04-image%EC%83%9D%EC%84%B1-%EC%97%85%EB%A1%9C%EB%93%9C.html</link>
        <guid isPermaLink="true">/ds/docker/2020/09/01/Docker-04-image%EC%83%9D%EC%84%B1-%EC%97%85%EB%A1%9C%EB%93%9C.html</guid>
        <description>&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://zzsza.github.io/development/2018/04/17/docker-kubernetes/&quot;&gt;Docker와 쿠버네티스의 이해&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Sep 2020 00:00:00 +0900</pubDate>
      </item>
    
  </channel>
</rss>
