<!DOCTYPE html>
<head>
   <!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true,
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
</script>
<script
  type="text/javascript"
  async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"
></script> -->

<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } });
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["$$","$$"] ],
        processEscapes: true
      }
    });
  </script>
  <script
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    type="text/javascript"
  ></script>
</head>
 
</head>
<html class="no-js">
  <head>
  <meta charset="utf-8" />
  <title>
    [DL Specialization] 02: Planar data classification with onehidden layer v6c | ì™„ìˆ™ì˜ ì—ê·¸ë¨¸ë‹ˆ ğŸ³
  </title>
  <meta
    name="description"
    content="hidden layerë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•´ë³´ì."
  />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Open Graph -->
  <!--  -->
  <meta
    property="og:title"
    content="[DL Specialization] 02: Planar data classification with onehidden layer v6c | ì™„ìˆ™ì˜ ì—ê·¸ë¨¸ë‹ˆ ğŸ³"
  />
  <meta
    property="og:description"
    content="hidden layerë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•´ë³´ì."
  />
  <meta property="og:type" content="website" />
  <meta
    property="og:url"
    content="http://localhost:4000/ds/dl/2020/11/04/coursera-02-Planar_data_classification_with_onehidden_layer_v6c.html"
  />
  <meta
    property="og:image"
    content="http://localhost:4000/assets/img/31.jpg"
  />

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!--Favicon-->
  <link
    rel="shortcut icon"
    href="/assets/favicon.ico"
    type="image/x-icon"
  />

  <!-- Canonical -->
  <link
    rel="canonical"
    href="/ds/dl/2020/11/04/coursera-02-Planar_data_classification_with_onehidden_layer_v6c.html"
  />

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="ì™„ìˆ™ì˜ ì—ê·¸ë¨¸ë‹ˆ ğŸ³" href="/feed.xml" />

  <!-- Font Awesome -->
  <link
    rel="stylesheet"
    href="https://use.fontawesome.com/releases/v5.8.2/css/all.css"
    integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay"
    crossorigin="anonymous"
  />

  <!-- Google Fonts -->
  
  <link
    href="//fonts.googleapis.com/css?family=Nanum+Gothic|Jua|Nanum+Gothic+Coding|Source+Code+Pro|Nanum+Myeongjo:400,700,800|Noto+Sans+KR:100,300,400,500,700,900|monospace"
    rel="stylesheet"
    type="text/css"
  />
  

  <!-- naver web mater tool -->
  <meta
    name="naver-site-verification"
    content="da86adfc2aa8cdd5f1d573b50497f3e29de44cd5"
  />

  <!-- KaTeX -->
  <!--
	
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
		integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

	<script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
		integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ"
		crossorigin="anonymous"></script>
	

    -->
  <!-- Google Analytics -->
  
  <script>
    (function (i, s, o, g, r, a, m) {
      i["GoogleAnalyticsObject"] = r;
      (i[r] =
        i[r] ||
        function () {
          (i[r].q = i[r].q || []).push(arguments);
        }),
        (i[r].l = 1 * new Date());
      (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m);
    })(
      window,
      document,
      "script",
      "//www.google-analytics.com/analytics.js",
      "ga"
    );

    ga("create", "UA-156961472-1", "auto");
    ga("send", "pageview");
  </script>
  

  <!-- Latex -->
  <!--
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
-->

   <!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true,
    }
  });
  MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
  MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
      alert("Math Processing Error: "+message[1]);
    });
</script>
<script
  type="text/javascript"
  async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"
></script> -->

<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } });
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["$$","$$"] ],
        processEscapes: true
      }
    });
  </script>
  <script
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    type="text/javascript"
  ></script>
</head>
  <script>
  console.log(window)

  window.addEventListener("resize", changeContent);
  window.addEventListener("load", changeContent);

  function changeContent(event){
    const width = window.innerWidth;
    console.log(event);
    const cat = document.getElementById("js-cat");
    const cv = document.getElementById("category-cv");
    const book= document.getElementById("category-book");
    const ds = document.getElementById("category-ds");
    const cs = document.getElementById("category-cs");
    const dv = document.getElementById("category-dv");
    const works = document.getElementById("category-works");
    const about = document.getElementById("category-about");
    if (width < 750) {
      cv.innerText = "ğŸ“œ"
      book.innerText = "ğŸ’¡"
      ds.innerText = "ğŸ“ˆ"
      cs.innerText = "ğŸ’¾"
      dv.innerText = "ğŸ“±"
      works.innerText = "ğŸ¨"
      about.innerText = "ğŸ˜"
      cat.style.justifyContent = "space-around"
    } else {
      cv.innerText = "CV"
      book.innerText = "Book"
      ds.innerText = "Data Science"
      cs.innerText = "Computer Science"
      dv.innerText = "Development"
      works.innerText = "Works"
      about.innerText = "About"
      cat.style.justifyContent = "space-around"
    }

  }
  
</script>
  <script>
  function handleSeriesNavShowing() {
    var side_nav = document.getElementById("js-side_nav");
    var side_nav_button_title = document.getElementById("js-side-btn-content");

    if (side_nav.style.display == "block") {
      side_nav.style.display = "none";
      side_nav_button_title.innerHTML = "â–¼ ëª©ë¡ ë³´ê¸°";
    } else {
      side_nav.style.display = "block";
      side_nav_button_title.innerHTML = "â–² ìˆ¨ê¸°ê¸°";
    }
  }
</script>
  <script>
  function handleTocNavShowing() {
    var toc_nav = document.getElementById("js-toc_nav");
    var toc_nav_button_title = document.getElementById("js-toc-btn-content");

    if (toc_nav.style.display == "block") {
      toc_nav.style.display = "none";
      toc_nav_button_title.innerHTML = "â–¼ ë‚´ë¦¬ê¸°";
    } else {
      toc_nav.style.display = "block";
      toc_nav_button_title.innerHTML = "â–² ìˆ¨ê¸°ê¸°";
    }
  }
</script>
 <script src="//code.jquery.com/jquery-3.3.1.min.js"></script>
<script>
  $(document).ready(function () {
    $(window).scroll(function () {
      if ($(this).scrollTop() > 200) {
        $(".top").fadeIn();
      } else {
        $(".top").fadeOut();
      }
    });

    $(".top").click(function () {
      $(".top").css("background", "#e7eceb");
      $(".top").css("color", "rgba(1, 53, 63, 0.8)");
      $("html, body").animate({ scrollTop: 0 }, 400);
      setTimeout(
        '$(".top").css("background", "rgba(1, 53, 63, 0.8)"); $(".top").css("color", "#e7eceb");',
        1000
      );
      return false;
    });
  });
</script>

</head>

  <body>
    <a href="#" class="top"></a>
 <header class="site-header">
  <div class="site-title-nav">
    <div class="branding">
      
      <a href="/">
        <img
          class="avatar"
          src="/assets/img/avatar.png"
          alt=""
        />
      </a>
      
      <h1 class="site-title">
        <a href="/">ì™„ìˆ™ì˜ ì—ê·¸ë¨¸ë‹ˆ ğŸ³</a>
      </h1>
    </div>
  </div>
  <nav class="site-nav">
    <ul>
      <!--
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                <li>
                    <a class="page-link" href="/about/">
                        About Me
                    </a>
                </li>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
-->
      <!-- Social icons from Font Awesome, if enabled  -->
      <!--                


<li>
	<a href="mailto:wansook0316@gmail.com" title="Email">
		<i class="fas fa-fw fa-envelope"></i>
	</a>
</li>













<li>
	<a href="https://github.com/wansook0316" title="Follow on GitHub">
		<i class="fab fa-fw fa-github"></i>
	</a>
</li>







<li>
	<a href="https://www.linkedin.com/in/wansik-choi-b065881aa/" title="Follow on LinkedIn">
		<i class="fab fa-fw fa-linkedin"></i>
	</a>
</li>























-->

      <!-- Search bar -->
      <li>


<li>
	<a href="mailto:wansook0316@gmail.com" title="Email">
		<i class="fas fa-fw fa-envelope"></i>
	</a>
</li>













<li>
	<a href="https://github.com/wansook0316" title="Follow on GitHub">
		<i class="fab fa-fw fa-github"></i>
	</a>
</li>







<li>
	<a href="https://www.linkedin.com/in/wansik-choi-b065881aa/" title="Follow on LinkedIn">
		<i class="fab fa-fw fa-linkedin"></i>
	</a>
</li>























</li>
      
      <li>
        <form action="/search.html" method="get">
          <input
            type="text"
            id="search-box"
            name="query"
            placeholder="Search"
            class=""
          />
          <button type="submit" class="">
            <i class="fa fa-fw fa-search"></i>
          </button>
        </form>
      </li>
      
    </ul>
  </nav>
</header>

<div class="site-category">
  <ul class="cat1" id="js-cat">
    <li>
      <a id="category-cv" href="/"> </a>
      <ul>
        <li><a href="/CV/Projects">Projects</a></li>
        <li><a href="/CV/Awards">Awards</a></li>
        <li><a href="/CV/Papers">Papers</a></li>
        <li><a href="/CV/ExtraAct">ExtraCurricular Act</a></li>
        <li><a href="/CV/InternShip">Internship</a></li>
        <li><a href="/CV/Language">Language</a></li>
        <li><a href="/CV/Courses">Courses</a></li>
        <li><a href="/CV/Education">Education</a></li>
        <li><a href="/CV/Qualification">Qualification</a></li>
      </ul>
    </li>

    <li>
      <a id="category-book" href="/"> </a>
      <ul>
        <li><a href="/Book/Finance">Finance</a></li>
        <li><a href="/Book/BM">Business/Marketing</a></li>
        <li><a href="/Book/SM">Self Management</a></li>
        <li><a href="/Book/CH">Culture/History</a></li>
        <li><a href="/Book/Leadership">Leadership</a></li>
        <li><a href="/Book/Classics">Classics</a></li>
        <li><a href="/Book/Think">Think</a></li>
      </ul>
    </li>

    <li>
      <a id="category-ds" href="/"> </a>
      <ul>
        <li><a href="/DS/DL">Deep Learning</a></li>
        <li><a href="/DS/ML">Machine Learning</a></li>
        <li><a href="/DS/Visualization">Visualization</a></li>
        <li><a href="/DS/Statistics">Statistics</a></li>
        <li><a href="/DS/LA">Linear Algebra</a></li>
        <li><a href="/DS/Calculus">Calculus</a></li>
      </ul>
    </li>

    <li>
      <a id="category-cs" href="/"> </a>
      <ul class="category-cs-ul">
        <li class="category-cs-li"><a href="/CS/Dynamics">Dynamics</a></li>
        <li class="category-cs-li"><a href="/CS/Network">Network</a></li>
        <li class="category-cs-li"><a href="/CS/Database">Database</a></li>
        <li class="category-cs-li"><a href="/CS/Algorithm">Algorithm</a></li>
        <li class="category-cs-li"><a href="/CS/Structure">Structure</a></li>
        <li class="category-cs-li"><a href="/CS/Parallel">Parallel</a></li>
        <li class="category-cs-li"><a href="/CS/OS">OS</a></li>
      </ul>
    </li>

    <li>
      <a id="category-dv" href="/"> </a>
      <ul class="category-dv-ul">
        <li class="category-dv-li"><a href="/DV/Server">Server</a></li>
        <li class="category-dv-li"><a href="/DV/Linux">Linux</a></li>
        <li class="category-dv-li"><a href="/DV/Python">Python</a></li>
        <li class="category-dv-li"><a href="/DV/C++">C++</a></li>
        <li class="category-dv-li"><a href="/DV/JavaScript">JavaScript</a></li>
        <li class="category-dv-li"><a href="/DV/HTML-CSS">HTML/CSS</a></li>
        <li class="category-dv-li"><a href="/DV/React">React</a></li>
        <li class="category-dv-li"><a href="/DV/Django">Django</a></li>
        <li class="category-dv-li"><a href="/DV/Docker">Docker</a></li>
        <li class="category-dv-li"><a href="/DV/Git">Git</a></li>
        <li class="category-dv-li"><a href="/DV/Tips">Tips</a></li>
        <li class="category-dv-li"><a href="/DV/SP">Side Project</a></li>
      </ul>
    </li>

    <li>
      <a id="category-works" href="/"> </a>
      <ul class="category-works-ul">
        <li class="category-works-li"><a href="/Works/Tags">Tags</a></li>
        <li class="category-works-li"><a href="/Works/Series">Series</a></li>
      </ul>
    </li>

    <li><a id="category-about" href="/about/"> </a></li>

    <!--    <ul class="cat2">-->

    <!--  </ul>-->
  </ul>
</div>

    <div class="content"><article
  class="feature-image main-category-feature"
  
>
  <header
    style="background-image: url('/assets/img/31.jpg')"
  >
    <ul class="category-title">
      <h1 class="title">02: Planar data classification with onehidden layer v6c</h1>
      
      <p class="meta">
        DS/DL  : DL Specialization 
      </p>
      <p class="meta">
        November 4, 2020 
      </p>
    </ul>
  </header>

  

  <section class="post-content">
         
<div class="series-nav">
  <p>
    ì´ í¬ìŠ¤íŒ…ì€ <b><em>DL Specialization</em></b> ì‹œë¦¬ì¦ˆ
    <b><em>2</em></b> í¸ ì¤‘ <b><em>2</em></b> ë²ˆì§¸ ê¸€ ì…ë‹ˆë‹¤.
  </p>
  <ul class="side-nav" id="js-side_nav">
                             
    <li onclick="location.href='/ds/dl/2020/11/04/coursera-01-Logistic_Regression_with_a_Neural_Network_mindset_v6a.html'">
      <span>Part 1 - 01: Logistic Regression with a Neural Network mindset v6a</span>
    </li>
                                                             
    <li class="current">
      <span>Part 2 - This Post</span>
    </li>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
  </ul>
  <div
    class="side-nav-btn"
    id="js-btn-side_nav"
    onclick="handleSeriesNavShowing();"
  >
    <div class="side-nav-btn__content" id="js-side-btn-content">
      â–¼ ëª©ë¡ ë³´ê¸°
    </div>
  </div>
</div>
 
   
<div class="toc-whole-nav">
  <p>ëª©ì°¨</p>
  <!-- <ul class="toc-nav" id="js-toc_nav"> -->
  <ul id="js-toc_nav" class="toc-nav">
  <li class="toc-item"><a href="#planar-data-classification-with-one-hidden-layer" class="toc-anchor">Planar data classification with one hidden layer</a>
    <ul class="submenu_class">
      <li class="toc-item"><a href="#1---packages" class="toc-anchor">1 - Packages</a></li>
      <li class="toc-item"><a href="#2---dataset" class="toc-anchor">2 - Dataset</a></li>
      <li class="toc-item"><a href="#3---simple-logistic-regression" class="toc-anchor">3 - Simple Logistic Regression</a></li>
      <li class="toc-item"><a href="#4---neural-network-model" class="toc-anchor">4 - Neural Network model</a></li>
      <li class="toc-item"><a href="#5-performance-on-other-datasets" class="toc-anchor">5) Performance on other datasets</a></li>
    </ul>
  </li>
</ul>
  <!-- </ul> -->
  <div class="toc-nav-btn" id="js-btn-toc_nav" onclick="handleTocNavShowing();">
    <div class="toc-nav-btn__content" id="js-toc-btn-content">â–¼ ë‚´ë¦¬ê¸°</div>
  </div>
</div>

 <h1 id="planar-data-classification-with-one-hidden-layer">Planar data classification with one hidden layer</h1>

<p>Welcome to your week 3 programming assignment. Itâ€™s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression.</p>

<p><strong>You will learn how to:</strong></p>

<ul>
  <li>hidden layer í•˜ë‚˜ë¥¼ ê°€ì§„ ì´ì§„ ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ ê²ƒì´ë‹¤.</li>
  <li>ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ í™œì„± í•¨ìˆ˜ë¡œ ì‚¬ìš©í•  ê²ƒì´ë‹¤.</li>
  <li>ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.</li>
  <li>forward, back propagationì„ êµ¬í˜„í•œë‹¤.</li>
</ul>

<h2 id="1---packages">1 - Packages</h2>

<p>Letâ€™s first import all the packages that you will need during this assignment.</p>

<ul>
  <li><a href="https://www.numpy.org/">numpy</a> is the fundamental package for scientific computing with Python.</li>
  <li><a href="http://scikit-learn.org/stable/">sklearn</a> provides simple and efficient tools for data mining and data analysis.</li>
  <li><a href="http://matplotlib.org">matplotlib</a> is a library for plotting graphs in Python.</li>
  <li>testCases provides some test examples to assess the correctness of your functions</li>
  <li>planar_utils provide various useful functions used in this assignment</li>
</ul>

<div class="prompt input_prompt">
In&nbsp;[62]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Package imports
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">testCases_v2</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span>
<span class="kn">from</span> <span class="nn">planar_utils</span> <span class="kn">import</span> <span class="n">plot_decision_boundary</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">load_planar_dataset</span><span class="p">,</span> <span class="n">load_extra_datasets</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># set a seed so that the results are consistent
</span></code></pre></div>  </div>

</div>

<h2 id="2---dataset">2 - Dataset</h2>

<p>First, letâ€™s get the dataset you will work on. The following code will load a â€œflowerâ€ 2-class dataset into variables <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">Y</code>.</p>

<div class="prompt input_prompt">
In&nbsp;[63]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_planar_dataset</span><span class="p">()</span>
</code></pre></div>  </div>

</div>

<p>Visualize the dataset using matplotlib. The data looks like a â€œflowerâ€ with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. In other words, we want the classifier to define regions as either red or blue.</p>

<div class="prompt input_prompt">
In&nbsp;[64]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize the data:
</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">);</span>
</code></pre></div>  </div>

</div>

<p><img src="/assets/img/Planar_data_classification_with_onehidden_layer_v6c_files/Planar_data_classification_with_onehidden_layer_v6c_6_0.png" alt="png" /></p>

<p>You have:</p>

<ul>
  <li>a numpy-array (matrix) X that contains your features (x1, x2)</li>
  <li>a numpy-array (vector) Y that contains your labels (red:0, blue:1).</li>
</ul>

<p>Lets first get a better sense of what our data is like.</p>

<p><strong>Exercise</strong>: ë°ì´í„° ì…‹ì˜ ê°œìˆ˜ê°€ ëª‡ ê°œì¸ì§€ ì•Œì•„ë³´ì.</p>

<p><strong>Hint</strong>: í•´ë‹¹ í–‰ë ¬ì˜ shapeë¥¼ ì•Œì•„ë³´ë©´ ëœë‹¤. <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html">(help)</a></p>

<div class="prompt input_prompt">
In&nbsp;[66]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### START CODE HERE ### (â‰ˆ 3 lines of code)
</span><span class="n">shape_X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">shape_Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># training set size
### END CODE HERE ###
</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'The shape of X is: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">shape_X</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'The shape of Y is: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">shape_Y</span><span class="p">))</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'I have m = </span><span class="si">%</span><span class="s">d training examples!'</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The shape of X is: (2, 400)
The shape of Y is: (1, 400)
I have m = 400 training examples!

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:55%">
  
  <tr>
    <td>**shape of X**</td>
    <td> (2, 400) </td> 
  </tr>
  
  <tr>
    <td>**shape of Y**</td>
    <td>(1, 400) </td> 
  </tr>
  
    <tr>
    <td>**m**</td>
    <td> 400 </td> 
  </tr>
  
</table>

<h2 id="3---simple-logistic-regression">3 - Simple Logistic Regression</h2>

<p>ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ê¸° ì „ì—, ê·¸ëƒ¥ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ëŒë ¸ì„ ë•Œ ì–´ë–¤ ê²°ê³¼ì¸ì§€ í™•ì¸í•´ë³´ì.</p>

<div class="prompt input_prompt">
In&nbsp;[67]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train the logistic regression classifier
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegressionCV</span><span class="p">();</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</code></pre></div>  </div>

</div>

<p>You can now plot the decision boundary of these models. Run the code below.</p>

<div class="prompt input_prompt">
In&nbsp;[68]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot the decision boundary for logistic regression
</span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Logistic Regression"</span><span class="p">)</span>

<span class="c1"># Print accuracy
</span><span class="n">LR_predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'Accuracy of logistic regression: </span><span class="si">%</span><span class="s">d '</span> <span class="o">%</span> <span class="nb">float</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">LR_predictions</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">LR_predictions</span><span class="p">))</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span> <span class="o">+</span>
       <span class="s">'</span><span class="si">% </span><span class="s">'</span> <span class="o">+</span> <span class="s">"(percentage of correctly labelled datapoints)"</span><span class="p">)</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)

</code></pre></div></div>

<p><img src="/assets/img/Planar_data_classification_with_onehidden_layer_v6c_files/Planar_data_classification_with_onehidden_layer_v6c_13_1.png" alt="png" /></p>

<p><strong>Expected Output</strong>:</p>

<table style="width:55%">
  <tr>
    <td>**Accuracy**</td>
    <td> 47% </td> 
  </tr>
  
</table>

<p><strong>Interpretation</strong>: í•´ë‹¹ ë°ì´í„° ì…‹ì€ ì„ í˜•ì ìœ¼ë¡œ êµ¬ë¶„ì´ ë¶ˆê°€í•˜ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì„ í˜• ë¶„ë¥˜ê¸°ê°€ ì˜ ì‘ë™í•˜ì§€ ì•Šì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<h2 id="4---neural-network-model">4 - Neural Network model</h2>

<p>ì˜ ì‘ë™í•˜ì§€ ì•Šì•˜ìœ¼ë‹ˆ ì´ì œ hidden layerë¥¼ ì‚¬ìš©í•´ì„œ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í•´ë³´ì.</p>

<p><strong>Here is our model</strong>:
<img src="https://user-images.githubusercontent.com/37871541/98064132-eb809e00-1e94-11eb-93cf-e5e5019a77b8.png" alt="image" class="center-small" /></p>

<p><strong>Mathematically</strong>:</p>

<p>For one example $x^{(i)}$:
<script type="math/tex">z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\tag{1}</script>
<script type="math/tex">a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}</script>
<script type="math/tex">z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\tag{3}</script>
<script type="math/tex">\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}</script>
<script type="math/tex">% <![CDATA[
y^{(i)}_{prediction} = \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5 \\ 0 & \mbox{otherwise } \end{cases}\tag{5} %]]></script></p>

<p>ëª¨ë“  exampleì— ëŒ€í•œ ì˜ˆì¸¡ê³¼ ì´ì— ëŒ€í•œ ë¹„ìš©ì„ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ,
<script type="math/tex">J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}</script></p>

<p><strong>Reminder</strong>: The general methodology to build a Neural Network is to: 1. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the modelâ€™s parameters 3. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent)</p>

<p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code class="language-plaintext highlighter-rouge">nn_model()</code>. Once youâ€™ve built <code class="language-plaintext highlighter-rouge">nn_model()</code> and learnt the right parameters, you can make predictions on new data.</p>

<h3 id="41---defining-the-neural-network-structure">4.1 - Defining the neural network structure</h3>

<p><strong>Exercise</strong>: Define three variables:</p>

<ul>
  <li>n_x: input layerì˜ í¬ê¸°</li>
  <li>n_h: hidden layerì˜ í¬ê¸° (4ë¡œ ê³ ì •!)</li>
  <li>n_y: output layerì˜ í¬ê¸°</li>
</ul>

<p><strong>Hint</strong>: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p>

<div class="prompt input_prompt">
In&nbsp;[70]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: layer_sizes
</span>
<span class="k">def</span> <span class="nf">layer_sizes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="s">"""
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)

    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """</span>
    <span class="c1">### START CODE HERE ### (â‰ˆ 3 lines of code)
</span>    <span class="n">n_x</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># size of input layer
</span>    <span class="n">n_h</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">n_y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># size of output layer
</span>    <span class="c1">### END CODE HERE ###
</span>    <span class="k">return</span> <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[71]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_assess</span><span class="p">,</span> <span class="n">Y_assess</span> <span class="o">=</span> <span class="n">layer_sizes_test_case</span><span class="p">()</span>
<span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">layer_sizes</span><span class="p">(</span><span class="n">X_assess</span><span class="p">,</span> <span class="n">Y_assess</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The size of the input layer is: n_x = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_x</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The size of the hidden layer is: n_h = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_h</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The size of the output layer is: n_y = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_y</span><span class="p">))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The size of the input layer is: n_x = 5
The size of the hidden layer is: n_h = 4
The size of the output layer is: n_y = 2

</code></pre></div></div>

<p><strong>Expected Output</strong> (these are not the sizes you will use for your network, they are just used to assess the function youâ€™ve just coded).</p>

<table style="width:55%">
  <tr>
    <td>**n_x**</td>
    <td> 5 </td> 
  </tr>
  
    <tr>
    <td>**n_h**</td>
    <td> 4 </td> 
  </tr>
  
    <tr>
    <td>**n_y**</td>
    <td> 2 </td> 
  </tr>
  
</table>

<h3 id="42---initialize-the-models-parameters">4.2 - Initialize the modelâ€™s parameters</h3>

<p><strong>Exercise</strong>: <code class="language-plaintext highlighter-rouge">initialize_parameters()</code>ë¥¼ êµ¬í˜„í•˜ì.</p>

<p><strong>Instructions</strong>:</p>

<ul>
  <li>Make sure your parametersâ€™ sizes are right. Refer to the neural network figure above if needed.</li>
  <li>You will initialize the weights matrices with random values.
    <ul>
      <li>Use: <code class="language-plaintext highlighter-rouge">np.random.randn(a,b) * 0.01</code> to randomly initialize a matrix of shape (a,b).</li>
    </ul>
  </li>
  <li>
    <p>You will initialize the bias vectors as zeros.</p>

    <ul>
      <li>Use: <code class="language-plaintext highlighter-rouge">np.zeros((a,b))</code> to initialize a matrix of shape (a,b) with zeros.</li>
    </ul>
  </li>
  <li>íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°ë¥¼ ì˜ ì¡°ì •í•´ì•¼ í•œë‹¤.</li>
  <li>weightsë¥¼ ëœë¤í•œ ê°’ìœ¼ë¡œ ì´ˆê¸°í™” í•´ì•¼ í•œë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">np.random.randn(a,b) * 0.01</code>ë¥¼ ì‚¬ìš©í•œë‹¤.</li>
      <li>ì´ ë•Œ 0.01ë¥¼ ê³±í•´ì£¼ëŠ” ì´ìœ ëŠ”, ë§Œì•½ í‘œì¤€í¸ì°¨ 1ì¸ ì •ê·œë¶„í¬ì—ì„œ ëœë¤í•œ ê°’ì„ ë½‘ì•˜ë‹¤ê³  ê°€ì •í–ˆì„ ë•Œ ì´ë¥¼ <code class="language-plaintext highlighter-rouge">sigmoid</code>, <code class="language-plaintext highlighter-rouge">tanh</code>ì™€ ê°™ì€ í™œì„±í•¨ìˆ˜ì— ë„£ì„ ê²½ìš° ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ëŠ” ë¬¸ì œê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤.</li>
      <li>ìœ„ì˜ ë‘ í™œì„± í•¨ìˆ˜ëŠ” 0ê·¼ì²˜ ì´ì™¸ì˜ êµ¬ê°„ì—ì„œëŠ” ê¸°ìš¸ê¸°ê°€ ë§¤ìš° ì‘ì€ íŠ¹ì§•ì„ ë³´ì´ê¸° ë•Œë¬¸ì— í° í‘œì¤€í¸ì°¨ë¥¼ ê°€ì§€ëŠ” ì •ê·œë¶„í¬ì—ì„œ ê°’ì„ ì¶”ì¶œí–ˆì„ ë•Œ, ë¹„êµì  ê·¸ë ˆë””ì–¸íŠ¸ ì†Œì‹¤ì´ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.</li>
    </ul>
  </li>
  <li>biasëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™” í•œë‹¤.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">np.zeros((a,b))</code> ë¥¼ ì‚¬ìš©í•œë‹¤.</li>
      <li>biasëŠ” ì‚¬ì‹¤ Nueral netì˜ í•µì‹¬ì ì¸ ë¶€ë¶„ì´ë¼ ë³´ê¸° ì–´ë µë‹¤.</li>
    </ul>
  </li>
</ul>

<p>ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— ëŒ€í•œ ë‚´ìš©ì€ <a href="https://reniew.github.io/13/">ì´ ê¸€</a>ì„ ì°¸ê³ í•˜ì.</p>

<div class="prompt input_prompt">
In&nbsp;[72]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: initialize_parameters
</span>
<span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">):</span>
    <span class="s">"""
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer

    Returns:
    params -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># we set up a seed so that your output matches ours although the initialization is random.
</span>
    <span class="c1">### START CODE HERE ### (â‰ˆ 4 lines of code)
</span>    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_x</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_h</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_x</span><span class="p">))</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">))</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">"W1"</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s">"b1"</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s">"W2"</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s">"b2"</span><span class="p">:</span> <span class="n">b2</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">parameters</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[73]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span> <span class="o">=</span> <span class="n">initialize_parameters_test_case</span><span class="p">()</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"W1 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W1"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"b1 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b1"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"W2 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W2"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"b2 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b2"</span><span class="p">]))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[ 0.]]

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:90%">
  <tr>
    <td>**W1**</td>
    <td> [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]] </td> 
  </tr>
  
  <tr>
    <td>**b1**</td>
    <td> [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]] </td> 
  </tr>
  
  <tr>
    <td>**W2**</td>
    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> 
  </tr>

  <tr>
    <td>**b2**</td>
    <td> [[ 0.]] </td> 
  </tr>
  
</table>

<h3 id="43---the-loop">4.3 - The Loop</h3>

<p><strong>Question</strong>: <code class="language-plaintext highlighter-rouge">forward_propagation()</code>ì„ êµ¬í˜„í•˜ì.</p>

<p><strong>Instructions</strong>:</p>

<ul>
  <li>Look above at the mathematical representation of your classifier.</li>
  <li>You can use the function <code class="language-plaintext highlighter-rouge">sigmoid()</code>. It is built-in (imported) in the notebook.</li>
  <li>You can use the function <code class="language-plaintext highlighter-rouge">np.tanh()</code>. It is part of the numpy library.</li>
  <li>The steps you have to implement are:
    <ol>
      <li>Retrieve each parameter from the dictionary â€œparametersâ€ (which is the output of <code class="language-plaintext highlighter-rouge">initialize_parameters()</code>) by using <code class="language-plaintext highlighter-rouge">parameters[".."]</code>.</li>
      <li>Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).</li>
    </ol>
  </li>
  <li>Values needed in the backpropagation are stored in â€œ<code class="language-plaintext highlighter-rouge">cache</code>â€. The <code class="language-plaintext highlighter-rouge">cache</code> will be given as an input to the backpropagation function.</li>
</ul>

<div class="prompt input_prompt">
In&nbsp;[74]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: forward_propagation
</span>
<span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="s">"""
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)

    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
    """</span>
    <span class="c1"># Retrieve each parameter from the dictionary "parameters"
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 4 lines of code)
</span>    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"W1"</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"b1"</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"W2"</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"b2"</span><span class="p">]</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="c1"># Implement Forward Propagation to calculate A2 (probabilities)
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 4 lines of code)
</span>    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="n">cache</span> <span class="o">=</span> <span class="p">{</span><span class="s">"Z1"</span><span class="p">:</span> <span class="n">Z1</span><span class="p">,</span>
             <span class="s">"A1"</span><span class="p">:</span> <span class="n">A1</span><span class="p">,</span>
             <span class="s">"Z2"</span><span class="p">:</span> <span class="n">Z2</span><span class="p">,</span>
             <span class="s">"A2"</span><span class="p">:</span> <span class="n">A2</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[75]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_assess</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">forward_propagation_test_case</span><span class="p">()</span>
<span class="n">A2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X_assess</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="c1"># Note: we use the mean here just to make sure that your output matches ours.
</span><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'Z1'</span><span class="p">])</span> <span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'A1'</span><span class="p">]),</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'Z2'</span><span class="p">]),</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'A2'</span><span class="p">]))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.262818640198 0.091999045227 -1.30766601287 0.212877681719

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:50%">
  <tr>
    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 </td> 
  </tr>
</table>

<p>ì—¬ê¸° ê¹Œì§€ êµ¬í˜„í–ˆë‹¤ë©´, ìš°ë¦¬ëŠ” $A^{[2]}$ë¥¼ êµ¬í˜„í–ˆë‹¤. íŒŒì´ì¬ ë³€ìˆ˜ë¡œëŠ” <code class="language-plaintext highlighter-rouge">A2</code>ë¡œ ë‚˜íƒ€ë‚´ì–´ì ¸ ìˆë‹¤. ì´ ì•ˆì—ëŠ” ê°ê°ì˜ exampleì— ëŒ€í•´ $a^{<a href="i">2</a>}$ê°€ ê³„ì‚° ëœë‹¤. ì´ì œ ìš°ë¦¬ëŠ” ì´ ê°’ë“¤ì˜ í‰ê·  costë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.</p>

<script type="math/tex; mode=display">J = - \frac{1}{m} \sum\limits_{i = 1}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}</script>

<p><strong>Exercise</strong>: Implement <code class="language-plaintext highlighter-rouge">compute_cost()</code> to compute the value of the cost $J$.</p>

<p><strong>Instructions</strong>:</p>

<ul>
  <li>There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented
$- \sum\limits_{i=0}^{m}  y^{(i)}\log(a^{<a href="i">2</a>})$:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logprobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A2</span><span class="p">),</span><span class="n">Y</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>                <span class="c1"># no need to use a for loop!
</span></code></pre></div></div>

<p>(you can use either <code class="language-plaintext highlighter-rouge">np.multiply()</code> and then <code class="language-plaintext highlighter-rouge">np.sum()</code> or directly <code class="language-plaintext highlighter-rouge">np.dot()</code>).<br />
Note that if you use <code class="language-plaintext highlighter-rouge">np.multiply</code> followed by <code class="language-plaintext highlighter-rouge">np.sum</code> the end result will be a type <code class="language-plaintext highlighter-rouge">float</code>, whereas if you use <code class="language-plaintext highlighter-rouge">np.dot</code>, the result will be a 2D numpy array. We can use <code class="language-plaintext highlighter-rouge">np.squeeze()</code> to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array). We can cast the array as a type <code class="language-plaintext highlighter-rouge">float</code> using <code class="language-plaintext highlighter-rouge">float()</code>.</p>

<div class="prompt input_prompt">
In&nbsp;[87]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: compute_cost
</span>
<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="s">"""
    Computes the cross-entropy cost given in equation (13)

    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    [Note that the parameters argument is not used in this function,
    but the auto-grader currently expects this parameter.
    Future version of this notebook will fix both the notebook
    and the auto-grader so that `parameters` is not needed.
    For now, please include `parameters` in the function signature,
    and also when invoking this function.]

    Returns:
    cost -- cross-entropy cost given equation (13)

    """</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of example
</span>
    <span class="c1"># Compute the cross-entropy cost
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 2 lines of code)
</span>    <span class="n">logprobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A2</span><span class="p">),</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A2</span><span class="p">),(</span><span class="mi">1</span><span class="o">-</span> <span class="n">Y</span><span class="p">))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>  <span class="c1"># makes sure cost is the dimension we expect.
</span>                                    <span class="c1"># E.g., turns [[17]] into 17
</span>    <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">cost</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[88]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A2</span><span class="p">,</span> <span class="n">Y_assess</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">compute_cost_test_case</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">"cost = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y_assess</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = 0.6930587610394646

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:55%">
  <tr>
    <td>**cost**</td>
    <td> 0.693058761... </td> 
  </tr>
  
</table>

<p>ì´ë²ˆì—” ì—­ì „íŒŒë¥¼ ê³„ì‚°í•´ë³´ì. ì—­ì „íŒŒ ê³„ì‚° ë°©ë²•ì€ ì°¬ì°¬íˆ ì½ì–´ë³¼ í•„ìš”ê°€ ìˆë‹¤.</p>

<p><strong>Question</strong>: Implement the function <code class="language-plaintext highlighter-rouge">backward_propagation()</code>.</p>

<p><strong>Instructions</strong>:
Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. Youâ€™ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.</p>

<!-- <img src="images/grad_summary.png" style="width:600px;height:300px;"> -->

<p><img src="https://user-images.githubusercontent.com/37871541/98064283-4e723500-1e95-11eb-92ef-92a4466cfa6c.png" alt="image" class="center-small" /></p>

<!--
$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$

$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $

$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$

$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $

$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $

$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$

- Note that $*$ denotes elementwise multiplication.
- The notation you will use is common in deep learning coding:
    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$
    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$
    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$
    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$

!-->

<ul>
  <li>Tips:
    <ul>
      <li>To compute dZ1 youâ€™ll need to compute $g^{[1]â€™}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]â€™}(z) = 1-a^2$. So you can compute
$g^{[1]â€™}(Z^{[1]})$ using <code class="language-plaintext highlighter-rouge">(1 - np.power(A1, 2))</code>.</li>
    </ul>
  </li>
</ul>

<div class="prompt input_prompt">
In&nbsp;[89]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: backward_propagation
</span>
<span class="k">def</span> <span class="nf">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="s">"""
    Implement the backward propagation using the instructions above.

    Arguments:
    parameters -- python dictionary containing our parameters
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
    X -- input data of shape (2, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)

    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    """</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># First, retrieve W1 and W2 from the dictionary "parameters".
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 2 lines of code)
</span>    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"W1"</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"W2"</span><span class="p">]</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="c1"># Retrieve also A1 and A2 from dictionary "cache".
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 2 lines of code)
</span>    <span class="n">A1</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">"A1"</span><span class="p">]</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">"A2"</span><span class="p">]</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="c1"># Backward propagation: calculate dW1, db1, dW2, db2.
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)
</span>    <span class="n">dZ2</span> <span class="o">=</span> <span class="n">A2</span><span class="o">-</span><span class="n">Y</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s">"dW1"</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span>
             <span class="s">"db1"</span><span class="p">:</span> <span class="n">db1</span><span class="p">,</span>
             <span class="s">"dW2"</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span>
             <span class="s">"db2"</span><span class="p">:</span> <span class="n">db2</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">grads</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[90]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X_assess</span><span class="p">,</span> <span class="n">Y_assess</span> <span class="o">=</span> <span class="n">backward_propagation_test_case</span><span class="p">()</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X_assess</span><span class="p">,</span> <span class="n">Y_assess</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"dW1 = "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s">"dW1"</span><span class="p">]))</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"db1 = "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s">"db1"</span><span class="p">]))</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"dW2 = "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s">"dW2"</span><span class="p">]))</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"db2 = "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s">"db2"</span><span class="p">]))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dW1 = [[ 0.00301023 -0.00747267]
 [ 0.00257968 -0.00641288]
 [-0.00156892  0.003893  ]
 [-0.00652037  0.01618243]]
db1 = [[ 0.00176201]
 [ 0.00150995]
 [-0.00091736]
 [-0.00381422]]
dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]
db2 = [[-0.16655712]]

</code></pre></div></div>

<p><strong>Expected output</strong>:</p>

<table style="width:80%">
  <tr>
    <td>**dW1**</td>
    <td> [[ 0.00301023 -0.00747267]
 [ 0.00257968 -0.00641288]
 [-0.00156892  0.003893  ]
 [-0.00652037  0.01618243]] </td> 
  </tr>
  
  <tr>
    <td>**db1**</td>
    <td>  [[ 0.00176201]
 [ 0.00150995]
 [-0.00091736]
 [-0.00381422]] </td> 
  </tr>
  
  <tr>
    <td>**dW2**</td>
    <td> [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]] </td> 
  </tr>

  <tr>
    <td>**db2**</td>
    <td> [[-0.16655712]] </td> 
  </tr>
  
</table>

<p><strong>Question</strong>: ì—…ë°ì´íŠ¸ ê·œì¹™ì„ êµ¬í˜„í•˜ì. ê·¸ë ˆë””ì–¸íŠ¸ ê°ì†Œ ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p>

<p><strong>General gradient descent rule</strong>: $ \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$ where $\alpha$ is the learning rate and $\theta$ represents a parameter.</p>

<p><strong>Illustration</strong>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</p>

<p><img src="https://user-images.githubusercontent.com/37871541/98064363-7a8db600-1e95-11eb-8297-105dd874a020.png" style="width:400;height:400;" />{ .center_small}
<img src="https://user-images.githubusercontent.com/37871541/98064373-80839700-1e95-11eb-8c4b-ebe84522ba6e.png" style="width:400;height:400;" /></p>

<p><img src="https://user-images.githubusercontent.com/37871541/98064363-7a8db600-1e95-11eb-8297-105dd874a020.png" alt="image" />
<img src="https://user-images.githubusercontent.com/37871541/98064373-80839700-1e95-11eb-8c4b-ebe84522ba6e.png" alt="image" /></p>

<div class="prompt input_prompt">
In&nbsp;[91]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: update_parameters
</span>
<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">):</span>
    <span class="s">"""
    Updates parameters using the gradient descent update rule given above

    Arguments:
    parameters -- python dictionary containing your parameters
    grads -- python dictionary containing your gradients

    Returns:
    parameters -- python dictionary containing your updated parameters
    """</span>
    <span class="c1"># Retrieve each parameter from the dictionary "parameters"
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 4 lines of code)
</span>    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"W1"</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"b1"</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"W2"</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"b2"</span><span class="p">]</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="c1"># Retrieve each gradient from the dictionary "grads"
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 4 lines of code)
</span>    <span class="n">dW1</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">"dW1"</span><span class="p">]</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">"db1"</span><span class="p">]</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">"dW2"</span><span class="p">]</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s">"db2"</span><span class="p">]</span>
    <span class="c1">## END CODE HERE ###
</span>
    <span class="c1"># Update rule for each parameter
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 4 lines of code)
</span>    <span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">dW1</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">db1</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">dW2</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">db2</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">"W1"</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s">"b1"</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s">"W2"</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s">"b2"</span><span class="p">:</span> <span class="n">b2</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">parameters</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[92]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">update_parameters_test_case</span><span class="p">()</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"W1 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W1"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"b1 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b1"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"W2 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W2"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"b2 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b2"</span><span class="p">]))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[ -1.02420756e-06]
 [  1.27373948e-05]
 [  8.32996807e-07]
 [ -3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[ 0.00010457]]

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:80%">
  <tr>
    <td>**W1**</td>
    <td> [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]</td> 
  </tr>
  
  <tr>
    <td>**b1**</td>
    <td> [[ -1.02420756e-06]
 [  1.27373948e-05]
 [  8.32996807e-07]
 [ -3.20136836e-06]]</td> 
  </tr>
  
  <tr>
    <td>**W2**</td>
    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> 
  </tr>

  <tr>
    <td>**b2**</td>
    <td> [[ 0.00010457]] </td> 
  </tr>
  
</table>

<h3 id="44---integrate-parts-41-42-and-43-in-nn_model">4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()</h3>

<p><strong>Question</strong>: ìœ„ì—ì„œ ì œì‘í•œ í•¨ìˆ˜ë“¤ì„ ì¡°í•©í•´ì„œ <code class="language-plaintext highlighter-rouge">nn_model()</code>ì„ ë§Œë“¤ì.</p>

<p><strong>Instructions</strong>: The neural network model has to use the previous functions in the right order.</p>

<div class="prompt input_prompt">
In&nbsp;[93]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: nn_model
</span>
<span class="k">def</span> <span class="nf">nn_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">"""
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations

    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">n_x</span> <span class="o">=</span> <span class="n">layer_sizes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_y</span> <span class="o">=</span> <span class="n">layer_sizes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># Initialize parameters
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 1 line of code)
</span>    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="c1"># Loop (gradient descent)
</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>

        <span class="c1">### START CODE HERE ### (â‰ˆ 4 lines of code)
</span>        <span class="c1"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
</span>        <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

        <span class="c1"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".
</span>        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

        <span class="c1"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
</span>        <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

        <span class="c1"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
</span>        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>

        <span class="c1">### END CODE HERE ###
</span>
        <span class="c1"># Print the cost every 1000 iterations
</span>        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="s">"Cost after iteration </span><span class="si">%</span><span class="s">i: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">parameters</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[94]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_assess</span><span class="p">,</span> <span class="n">Y_assess</span> <span class="o">=</span> <span class="n">nn_model_test_case</span><span class="p">()</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">nn_model</span><span class="p">(</span><span class="n">X_assess</span><span class="p">,</span> <span class="n">Y_assess</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"W1 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W1"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"b1 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b1"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"W2 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W2"</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"b2 = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b2"</span><span class="p">]))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cost after iteration 0: 0.692739
Cost after iteration 1000: 0.000218
Cost after iteration 2000: 0.000107
Cost after iteration 3000: 0.000071
Cost after iteration 4000: 0.000053
Cost after iteration 5000: 0.000042
Cost after iteration 6000: 0.000035
Cost after iteration 7000: 0.000030
Cost after iteration 8000: 0.000026
Cost after iteration 9000: 0.000023
W1 = [[-0.65848169  1.21866811]
 [-0.76204273  1.39377573]
 [ 0.5792005  -1.10397703]
 [ 0.76773391 -1.41477129]]
b1 = [[ 0.287592  ]
 [ 0.3511264 ]
 [-0.2431246 ]
 [-0.35772805]]
W2 = [[-2.45566237 -3.27042274  2.00784958  3.36773273]]
b2 = [[ 0.20459656]]

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:90%">

<tr> 
    <td> 
        **cost after iteration 0**
    </td>
    <td> 
        0.692739
    </td>
</tr>

<tr> 
    <td> 
        <center> $\vdots$ </center>
    </td>
    <td> 
        <center> $\vdots$ </center>
    </td>
</tr>

  <tr>
    <td>**W1**</td>
    <td> [[-0.65848169  1.21866811]
 [-0.76204273  1.39377573]
 [ 0.5792005  -1.10397703]
 [ 0.76773391 -1.41477129]]</td> 
  </tr>
  
  <tr>
    <td>**b1**</td>
    <td> [[ 0.287592  ]
 [ 0.3511264 ]
 [-0.2431246 ]
 [-0.35772805]] </td> 
  </tr>
  
  <tr>
    <td>**W2**</td>
    <td> [[-2.45566237 -3.27042274  2.00784958  3.36773273]] </td> 
  </tr>

  <tr>
    <td>**b2**</td>
    <td> [[ 0.20459656]] </td> 
  </tr>
  
</table>

<h3 id="45-predictions">4.5 Predictions</h3>

<p><strong>Question</strong>: Use your model to predict by building predict().
Use forward propagation to predict results.</p>

<p><strong>Reminder</strong>: predictions = $y_{prediction} = \mathbb 1 \text = \begin{cases}
      1 &amp; \text{if}\ activation &gt; 0.5 <br />
      0 &amp; \text{otherwise}
    \end{cases}$</p>

<p>As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: <code class="language-plaintext highlighter-rouge">X_new = (X &gt; threshold)</code></p>

<div class="prompt input_prompt">
In&nbsp;[95]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GRADED FUNCTION: predict
</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="s">"""
    Using the learned parameters, predicts a class for each example in X

    Arguments:
    parameters -- python dictionary containing your parameters
    X -- input data of size (n_x, m)

    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    """</span>

    <span class="c1"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
</span>    <span class="c1">### START CODE HERE ### (â‰ˆ 2 lines of code)
</span>    <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">A2</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###
</span>
    <span class="k">return</span> <span class="n">predictions</span>
</code></pre></div>  </div>

</div>

<div class="prompt input_prompt">
In&nbsp;[96]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parameters</span><span class="p">,</span> <span class="n">X_assess</span> <span class="o">=</span> <span class="n">predict_test_case</span><span class="p">()</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X_assess</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"predictions mean = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="p">)))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>predictions mean = 0.666666666667

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:40%">
  <tr>
    <td>**predictions mean**</td>
    <td> 0.666666666667 </td> 
  </tr>
  
</table>

<p>It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units.</p>

<div class="prompt input_prompt">
In&nbsp;[97]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Build a model with a n_h-dimensional hidden layer
</span><span class="n">parameters</span> <span class="o">=</span> <span class="n">nn_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_h</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Plot the decision boundary
</span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Decision Boundary for hidden layer size "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cost after iteration 0: 0.693048
Cost after iteration 1000: 0.288083
Cost after iteration 2000: 0.254385
Cost after iteration 3000: 0.233864
Cost after iteration 4000: 0.226792
Cost after iteration 5000: 0.222644
Cost after iteration 6000: 0.219731
Cost after iteration 7000: 0.217504
Cost after iteration 8000: 0.219471
Cost after iteration 9000: 0.218612

</code></pre></div></div>

<div class="language-plaintext output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.text.Text at 0x7f3e5cf62438&gt;
</code></pre></div></div>

<p><img src="/assets/img/Planar_data_classification_with_onehidden_layer_v6c_files/Planar_data_classification_with_onehidden_layer_v6c_50_2.png" alt="png" /></p>

<p><strong>Expected Output</strong>:</p>

<table style="width:40%">
  <tr>
    <td>**Cost after iteration 9000**</td>
    <td> 0.218607 </td> 
  </tr>
  
</table>

<div class="prompt input_prompt">
In&nbsp;[98]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print accuracy
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">'Accuracy: </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="nb">float</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy: 90%

</code></pre></div></div>

<p><strong>Expected Output</strong>:</p>

<table style="width:15%">
  <tr>
    <td>**Accuracy**</td>
    <td> 90% </td> 
  </tr>
</table>

<p>Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression.</p>

<p>Now, letâ€™s try out several hidden layer sizes.</p>

<h3 id="46---tuning-hidden-layer-size-optionalungraded-exercise">4.6 - Tuning hidden layer size (optional/ungraded exercise)</h3>

<p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p>

<div class="prompt input_prompt">
In&nbsp;[59]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This may take about 2 minutes to run
</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Hidden Layer of size </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="n">n_h</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">nn_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
    <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">"Accuracy for {} hidden units: {} </span><span class="si">%</span><span class="s">"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span>
</code></pre></div>  </div>

</div>

<div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy for 1 hidden units: 67.5 %
Accuracy for 2 hidden units: 67.25 %
Accuracy for 3 hidden units: 90.75 %
Accuracy for 4 hidden units: 90.5 %
Accuracy for 5 hidden units: 91.25 %
Accuracy for 20 hidden units: 90.0 %
Accuracy for 50 hidden units: 90.25 %

</code></pre></div></div>

<p><img src="/assets/img/Planar_data_classification_with_onehidden_layer_v6c_files/Planar_data_classification_with_onehidden_layer_v6c_56_1.png" alt="png" class="center" /></p>

<p><strong>Interpretation</strong>:</p>

<ul>
  <li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data.</li>
  <li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticeable overfitting.</li>
  <li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting.</li>
</ul>

<p><strong>Optional questions</strong>:</p>

<p><strong>Note</strong>: Remember to submit the assignment by clicking the blue â€œSubmit Assignmentâ€ button at the upper-right.</p>

<p>Some optional/ungraded questions that you can explore if you wish:</p>

<ul>
  <li>What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?</li>
  <li>Play with the learning_rate. What happens?</li>
  <li>What if we change the dataset? (See part 5 below!)</li>
</ul>

<p><strong>Youâ€™ve learnt to:</strong></p>

<ul>
  <li>Build a complete neural network with a hidden layer</li>
  <li>Make a good use of a non-linear unit</li>
  <li>Implemented forward propagation and backpropagation, and trained a neural network</li>
  <li>See the impact of varying the hidden layer size, including overfitting.</li>
</ul>

<p>Nice work!</p>

<h2 id="5-performance-on-other-datasets">5) Performance on other datasets</h2>

<p>If you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.</p>

<div class="prompt input_prompt">
In&nbsp;[60]:
</div>

<div class="input_area">

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Datasets
</span><span class="n">noisy_circles</span><span class="p">,</span> <span class="n">noisy_moons</span><span class="p">,</span> <span class="n">blobs</span><span class="p">,</span> <span class="n">gaussian_quantiles</span><span class="p">,</span> <span class="n">no_structure</span> <span class="o">=</span> <span class="n">load_extra_datasets</span><span class="p">()</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">{</span><span class="s">"noisy_circles"</span><span class="p">:</span> <span class="n">noisy_circles</span><span class="p">,</span>
            <span class="s">"noisy_moons"</span><span class="p">:</span> <span class="n">noisy_moons</span><span class="p">,</span>
            <span class="s">"blobs"</span><span class="p">:</span> <span class="n">blobs</span><span class="p">,</span>
            <span class="s">"gaussian_quantiles"</span><span class="p">:</span> <span class="n">gaussian_quantiles</span><span class="p">}</span>

<span class="c1">### START CODE HERE ### (choose your dataset)
</span><span class="n">dataset</span> <span class="o">=</span> <span class="s">"noisy_moons"</span>
<span class="c1">### END CODE HERE ###
</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="n">dataset</span><span class="p">]</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># make blobs binary
</span><span class="k">if</span> <span class="n">dataset</span> <span class="o">==</span> <span class="s">"blobs"</span><span class="p">:</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">%</span><span class="mi">2</span>

<span class="c1"># Visualize the data
</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">);</span>
</code></pre></div>  </div>

</div>

<p><img src="/assets/img/Planar_data_classification_with_onehidden_layer_v6c_files/Planar_data_classification_with_onehidden_layer_v6c_63_0.png" alt="png" /></p>

<p>Congrats on finishing this Programming Assignment!</p>

<p>Reference:</p>

<ul>
  <li>http://scs.ryerson.ca/~aharley/neural-networks/</li>
  <li>http://cs231n.github.io/neural-networks-case-study/</li>
</ul>

  </section>
  
<footer>
  <div class="tags">
    
    <a
      class="tag"
      href="http://localhost:4000/Works/tags#test"
      >#test</a
    >
    
  </div>
</footer>


</article>

<!-- Post navigation -->
<div id="post-nav">
           
                          
  <div
    id="previous-post"
    class="post-nav-post"
    onclick="location.href='/ds/dl/2020/11/04/coursera-01-Logistic_Regression_with_a_Neural_Network_mindset_v6a.html';"
  >
    <p>PREVIOUS SERIES</p>
    <p>01: Logistic Regression with a Neural Network mindset v6a</p>
  </div>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
</div>


<!-- Disqus -->

<div class="comments"><div id="disqus_thread"></div>
<script type="text/javascript">
	var disqus_shortname = 'wansook0316';
	(function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view comments.</noscript>
</div>


<!-- Post navigation -->

</div>

    
<script src="/assets/js/katex_init.js"></script>






<footer class="site-footer">
	<div class="copyright">
		Â© 
		<span itemprop="copyrightYear">2018</span>
		<span class="with-love">
		  <i class="fa fa-heart"></i>
		</span>
		<span class="author" itemprop="copyrightHolder">Wansook</span>
	  </div>

	<div class="powered-by">
	Powered by <a class="theme-link" href="https://jekyllrb.com" >Jekyll</a>
	</div>

	<div class="prev-blog-info">
		Prev Blog -
		<a class="theme-link" href="https://egg-money.tistory.com/127?category=822389">
		  ì™„ìˆ™ì˜ ë¸”ë¡œê·¸
		</a>
	</div>

	<div class="saying">What I cannot create, I do not understand. <br> - Richard Phillips Feynman -</div>
</footer>

 <script>
  /**
   * Animate scrolling to a target position
   * @param {string} _selector Target selector
   * @param {number} _duration (Option) Duration time(ms) (Default. 800ms)
   * @param {number} _adjust (Option) Adjustment value of position
   */
  /**
  animteScrollTo = function (_selector, _duration, _adjust) {
    const targetEle = document.querySelector(_selector);
    if (!targetEle) return;

    // - Get current &amp; target positions
    const scrollEle = document.documentElement || window.scrollingElement,
      currentY = scrollEle.scrollTop,
      targetY = targetEle.offsetTop - (_adjust || 0);
    animateScrollTo(currentY, targetY, _duration);

    // - Animate and scroll to target position
    function animateScrollTo(_startY, _endY, _duration) {
      _duration = _duration ? _duration : 600;
      const unitY = (targetY - currentY) / _duration;
      const startTime = new Date().getTime();
      const endTime = new Date().getTime() + _duration;

      const scrollTo = function () {
        let now = new Date().getTime();
        let passed = now - startTime;
        if (now <= endTime) {
          scrollEle.scrollTop = currentY + unitY * passed;
          requestAnimationFrame(scrollTo);
        } else {
          console.log("End off.");
        }
      };
      requestAnimationFrame(scrollTo);
    }
  };

  anchorArray = Array.from(document.querySelectorAll(".toc-anchor"));
  console.log(anchorArray);
  for (var i = 0; i < anchorArray.length; i++) {
    (function (m) {
      anchorId = String(anchorArray[m].innerText)
        .toLowerCase()
        .replace(/\s/g, "-");
      // console.log(anchorId);
      target = $(
        '.toc-whole-nav .toc-nav .toc-item a[href="#' + anchorId + '"]'
      );
      console.log(target);
      // console.log(anchorArray[m]);
      target.click(function () {
        name = String(target[0].innerText).toLowerCase().replace(/\s/g, "-");
        // name = name.replace(/\./, "-");
        console.log("#" + name);
        animteScrollTo("#" + name);
      }, false);
    })(i);
  }
**/
  // anchorArray = Array.from(document.querySelectorAll(".toc-anchor"));
  // // console.log(anchorArray);

  // anchorArray.map((anchor) => {
  //   $(anchor).click(function () {
  //     console.log(anchor["innerText"]);
  //     name = anchor["innerText"];
  //     name = name.replace(/\./, "-");
  //     // console.log(name);

  //     animteScrollTo("#" + name);
  //   });
  //   return anchor;
  // });

  // var headings = $("h1, h2, h3, h4, h5, h6").toArray()
  //         , headingToListElementLinkMap = getHeadingToListElementLinkMap(headings)
  //         , listElementLinks = $.map(headingToListElementLinkMap, (function(value, key) {
  //           return value
  //       }
  //       ))
  //         , scrollOffset = getScrollOffset();

  //   function getEzTocListElementLinkByHeading(heading) {
  //     return $('.toc-whole-nav .toc-nav .toc-item a[href="#' + $(heading).attr("id") + '"]')
  //   }

  //   function getHeadingToListElementLinkMap(headings) {
  //           return headings.reduce((function(map, heading) {
  //               return map[heading.id] = getEzTocListElementLinkByHeading(heading),
  //               map
  //           }
  //           ), {})
  //       }
  //   });
  //   function getScrollOffset() {
  //     var scrollOffset = 5;
  //     void 0 !== ezTOC.smooth_scroll && 1 === parseInt(ezTOC.smooth_scroll) && (scrollOffset = void 0 !== ezTOC.scroll_offset ? parseInt(ezTOC.scroll_offset) : 30);
  //     var adminbar = $("#wpadminbar");
  //     return adminbar.length && (scrollOffset += adminbar.height()),
  //     scrollOffset
  //   }

  // $(document).ready(function() {
  //   $('.user-list').append(_.map(users, function(user) {
  //     var button = $('<button type="button" class="btn btn-primary">').text(user.name);
  //     button.click(function() {
  //         if (confirm(user.name + "ë‹˜ì„íŒ”ë¡œì‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")) follow(user);
  //     });
  //     return button;
  //   }));
  // });

  // function follow(user) {
  //     alert("ì´ì œ " + user.name + "ë‹˜ì˜ì†Œì‹ì„ë³´ì‹¤ìˆ˜ìˆìŠµë‹ˆë‹¤.");
  // }
  // str.replace(/\./, "-")
  // anchorArray = Array.from(document.querySelectorAll(".toc-anchor"));
  // console.log(anchorArray);
  // anchorArray.map((anchor) => {
  //   console.log(anchor);
  //   // $(anchor).click(function () {
  //   //   console.log($("anchor"));
  //   // });
  //   // return anchor;
  // });

  //   var t = document.getElementById("target");
  //   const toc = document.querySelectorAll(".toc_nav > ul > li");
  //   console.log(toc);
  // t.addEventListener('click', function(event){
  //     alert('Hello world, '+event.target.value);
  // });
</script>

  </body>
</html>
